{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775a8a63-df82-4cae-b9d1-f28fcec4f71b",
   "metadata": {},
   "source": [
    "Welcome to the laboratory computers for the course \"Neural signals and signal processing\". The aim of the laboratories is to provide insights on how to analyze other kinds of imaging data: here in particular we will look at functional Near Infrared Spectroscopy (fNRIS) and diffusion-weighted MRI to generate tractography and a structural connectome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585ed142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os.path as op\n",
    "import mne\n",
    "import mne_nirs\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.preprocessing.nirs import temporal_derivative_distribution_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2321d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><b>Please remember to run this notebook always with the command: \"fsleyes --notebookFile lab_3_fNIRs_tracto.ipynb\"! </b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005f9d3-e329-4837-918e-9df0b384489e",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "<h1><font color='black'>Part 1: Get acquainted with fNIRS data preprocessing </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a5f92-f5c6-419f-8e53-5ce8922d4a22",
   "metadata": {},
   "source": [
    "In contrast to functional magnetic resonance imaging (fMRI), functional Near-Infrared Spectroscopy (fNIRS) distinguishes itself with portability and adaptability, making it especially suitable for research involving intricate populations like infants, tasks characterized by motion, and real-world settings.\n",
    "\n",
    "Nevertheless, it is important to acknowledge that collecting fNIRS data requires rigorous preprocessing. This requirement arises from fNIRS's susceptibility to 1) superficial physiological interferences, such as those stemming from scalp blood flow and 2) motion artifacts, especially those resulting from sensor displacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16aba2c",
   "metadata": {},
   "source": [
    "# 1. Visualization</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b9df4",
   "metadata": {},
   "source": [
    "Effective data preprocessing begins with data visualization to discern the desired and undesired variability (keep in mind the 1-10-100 dollar rule). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ddcbc-ad58-46b9-99e1-955885332020",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3c43f",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3790ca-1acb-48e1-bb57-61c2d4200991",
   "metadata": {},
   "source": [
    "We will work on a set of hemodynamic data measured on one subject during a finger tapping paradigm with three conditions: 1) Tapping the left thumb to fingers, 2) Tapping the right thumb to fingers and 3) A control when nothing happens. Each tapping lasts 5 seconds and there are 30 trials in each condition.The recording was performed using fNIRS sensors located over motor areas of the cortex. \n",
    "\n",
    "Data were provided by Luke, R., & McAlpine, D. (2021). fNIRS Finger Tapping Data in BIDS Format (Version v0.0.1) (https://doi.org/10.5281/zenodo.5529797),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5d71",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eca6c2-6752-42d0-8a72-c641a7172cd4",
   "metadata": {},
   "source": [
    "Load the dataset by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7319ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset\n",
    "fnirs_data_folder = mne.datasets.fnirs_motor.data_path()\n",
    "#Get the path for the dataset folder \n",
    "fnirs_data_folder=op.join(fnirs_data_folder, 'Participant-1')\n",
    "#Load the dataset \n",
    "raw_intensity = mne.io.read_raw_nirx(fnirs_data_folder, verbose=True, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b267d19",
   "metadata": {},
   "source": [
    "#### Recording setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c16446-b605-4aef-b3d6-c4481d1d2b34",
   "metadata": {},
   "source": [
    "Display and read information on the recording setting such as the number of channels, the file duration and the sampling frequency by runing the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa256c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display recording setting\n",
    "raw_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0cf60-9d52-47b2-968e-1119147d627a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Multiple Choice Question</span>\n",
    "\n",
    "*Among the choices below, what is the shape of the measured dataset (number of fNIRS channels, timepoints) ?*\n",
    "   1. (56, 23235)\n",
    "   2. (17, 98888)\n",
    "   3. (56, 78889) \n",
    "   4. (17, 23235)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d3ad9-382b-49ab-9d4a-4a9becbc7bb9",
   "metadata": {},
   "source": [
    "The placement of fNIRS sensors holds significance for achieving **a good spatial resolution** and an **accurate sensor placement**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374edb6c",
   "metadata": {},
   "source": [
    "Let's take a look at the locations of sensors. Before that, you need to install two additional packages using the following terminal commands:\n",
    "\n",
    "'pip install pyvistaqt'\n",
    "\n",
    "\n",
    "'pip install ipywidgets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_dir = mne.datasets.sample.data_path() / \"subjects\"\n",
    "\n",
    "brain = mne.viz.Brain(\n",
    "    \"fsaverage\", subjects_dir=subjects_dir, background=\"w\", cortex=\"0.5\"\n",
    ")\n",
    "brain.add_sensors(\n",
    "    raw_intensity.info,\n",
    "    trans=\"fsaverage\",\n",
    "    fnirs=[\"channels\", \"pairs\", \"sources\", \"detectors\"],\n",
    ")\n",
    "brain.show_view(azimuth=20, elevation=60, distance=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08096f8",
   "metadata": {},
   "source": [
    "In the 3D visualization, we represent source-detector pairs as lines. Sources are shown as red dots and detectors as black dots. The midpoint of these pairs, which we call channels, is depicted as orange dots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23aec-dbc9-458b-a5de-1bd8f32cb952",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Multiple Choice Question</span>\n",
    "\n",
    "*What is the cortical region covered by this measurement ?*\n",
    "1. Motor cortex\n",
    "2. Visual cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc93e0b",
   "metadata": {},
   "source": [
    "#### Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c02f5-2d5a-44e7-9c09-9b17f0690c1b",
   "metadata": {},
   "source": [
    "Let's now have a look at the experimental design used.\n",
    "\n",
    "The experimental designs most commonly employed by auditory fNIRS researchers are block- and event-related designs. In an event related design, each task is presented individually for a short amount of time e.g., 3 seconds. In this way, tasks can be more  randomized,  rather  than  being  blocked  together  by  condition. Conversely, in a block-related design, blocks of tasks, each lasting at least 10 seconds, recur before intervals of rest. \n",
    "\n",
    "The choice of the experimental design depends on a range of factors, including the statistical power, the duration of the experiment, and whether the design provides the flexibility to study the effect of interest. While the block design might lead to higher detection power, it can also induce learning and boredom effects which may bias the results. On the other hand, event-related designs reduce the effects of learning, boredom while exhibiting loss in detection power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd3413",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Open question</span>\n",
    "\n",
    "*What are the analytical challenges associated with employing a block-related experimental design featuring continuous stimulation and brief rest blocks?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6d506-59bc-4300-b1ec-e0b55747c4f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Run the next cell to display the sequence of events used in this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9690b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the event annotations\n",
    "events, event_dict=mne.events_from_annotations(raw_intensity,verbose=False)\n",
    "#Assign each label to the event (based on recording setting)\n",
    "event_dict={'Control':1,'Tapping/Left':4,'Tapping/Right':3,'ExperimentEnds':2}\n",
    "#Display the sequence of events \n",
    "plt.rcParams[\"figure.figsize\"]=(10,6)\n",
    "mne.viz.plot_events(events,event_id=event_dict,sfreq=raw_intensity.info['sfreq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213d4ad-e49f-4cf3-b4ab-c4ef1cb6ef0a",
   "metadata": {},
   "source": [
    "As you can see, there is a significant gap or interstimulus interval between each individual stimulus. We can thus conclude that an event-related design was employed in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2a3f7-feec-4b81-8128-9ed70614ab17",
   "metadata": {},
   "source": [
    "Now that we have gained an understanding of the experiment and recording processes, we can start the visualization of the raw data collected. To proceed, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure window\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "#Plot a time-series of raw data \n",
    "mne.viz.plot_raw(raw_intensity,start=120,duration=80,n_channels=56,show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687ea7c",
   "metadata": {},
   "source": [
    "In this plot, we can observe the time-series of the data collected, represented as the change in light intensity resulting from light absorption at specific wavelengths. Oxygenated hemoglobin (HbO) absorbs light at \"850\"nm, while deoxygenated hemoglobin (HbR) absorbs light at \"760\"nm. Each channel comprises a pair consisting of a source (labeled as \"S\" in the plot) and a detector (labeled as \"D\" in the plot). \n",
    "\n",
    "Here, we are showing all the channels (y-axis) but, for clarity, we are only focusing on a specific timeframe, spanning from 120 to 200 seconds (x-axis). If you wish to customize the visualization parameters, please refer to the documentation of [mne.viz.plot_raw](https://mne.tools/stable/generated/mne.viz.plot_raw.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33907f00",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Open question</span>\n",
    "\n",
    "*Visually, are you able to localize a motion artifact?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23001d",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcd271",
   "metadata": {},
   "source": [
    "A typical preprocessing pipeline for fNIRS data includes the following key steps:\n",
    "\n",
    "<hr>\n",
    "\n",
    "*Step 1: Raw Intensity to Optical Density Conversion*\n",
    "\n",
    "<hr> \n",
    "\n",
    "*Step 2: Motion Artifact Removal*\n",
    "\n",
    "<p style='margin-top:1em;'>💡 This step is more effective when performed at the beginning of the pipeline to prevent the propagation of errors across wavelengths and minimizes cross talk between signals obtained at different wavelengths.\n",
    "\n",
    "<hr> \n",
    "\n",
    "*Step 3: Optical Density to Concentration Conversion*\n",
    "\n",
    "<hr>\n",
    "\n",
    "*Step 4: Physiological Oscillation Filtering*\n",
    "\n",
    "<p style='margin-top:1em;'>💡 This step is more effective when performed after the conversion step using the modified Beer-Lambert law, as physiological sources of error impact HbO and HbR in a manner consistent with the Beer-Lambert equation.\n",
    "    \n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254195bd",
   "metadata": {},
   "source": [
    "#### Converting raw intensity data to optical density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b268b",
   "metadata": {},
   "source": [
    "The first step is to convert the changea in light intensity into changes in optical density using the modified Beer-Lambert law (Eq 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547df8e-0e72-45ec-8e38-4ac81da9f2fd",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "OD_\\lambda - OD_{R\\lambda} = log\\frac{I_o}{I} (1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e0262-9fb3-4f55-9644-d980fa138240",
   "metadata": {},
   "source": [
    "With:\n",
    "- $OD_\\lambda$ the optical density of the medium for a given wavelength $\\lambda$\n",
    "- $OD_{R\\lambda}$ the optical density of light scattering within human tissue\n",
    "- $I_o$ the incident light intensity \n",
    "- $I$ the transmitted light intensity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7931b-3449-4789-8ca1-49f54afa377e",
   "metadata": {},
   "source": [
    "Run the next cell to convert raw intensity into optical density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7e624f-b7bb-4f19-8da6-6480a56cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert raw intensity into optical density\n",
    "raw_od=mne.preprocessing.nirs.optical_density(raw_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541499a",
   "metadata": {},
   "source": [
    "#### Correcting motion artefacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921d012",
   "metadata": {},
   "source": [
    "In fNIRS data, two types of artifacts are commonly encountered: baseline shifts (indicating sensor displacement without returning to the initial position) and spike artifacts (characterized by sensor oscillations). As you correctly guessed earlier, the abrupt shift occurring around ~190 seconds in the time-series plot above corresponds to a spike artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5a755",
   "metadata": {},
   "source": [
    "Here, we will employ a technique known as Temporal Derivative Distribution Repair (TDDR), which is designed to address baseline shifts and spikes. TDDR uses the temporal derivative of the signal to detect baseline shifts and spikes which are then used to correct the data. \n",
    "\n",
    "For a more comprehensive understanding of the TDDR method and its application, you can refer to the following paper:\n",
    "- Frank A. Fishburn, Ruth S. Ludlum, Chandan J. Vaidya, and Andrei V. Medvedev. \"Temporal Derivative Distribution Repair (TDDR): A Motion Correction Method for fNIRS.\" NeuroImage, 184:171–179, 2019. doi:10.1016/j.neuroimage.2018.09.025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e26b3a",
   "metadata": {},
   "source": [
    "Run the cell below to apply the TDDR on the optical density data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd3c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_od_preproc=temporal_derivative_distribution_repair(raw_od)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346e30e",
   "metadata": {},
   "source": [
    "Execute the following cell to visualize the optical density data after applying TDDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure window\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "#Plot a time-series of light intensity \n",
    "mne.viz.plot_raw(raw_od_preproc,show_scrollbars=False,start=120,duration=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3546a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Open question</span>\n",
    "\n",
    "*From a visual standpoint, can we conclude that TDDR successfully eliminated the motion artifacts from your data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fad549",
   "metadata": {},
   "source": [
    "#### Converting optical density to concentration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3a0f4",
   "metadata": {},
   "source": [
    "The changes in optical density are then converted into changes of concentration using the modified beer lambert law (Eq 1): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ed4b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta c = \\frac{\\Delta OD_\\lambda }{\\epsilon_\\lambda lB} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91a1f2",
   "metadata": {},
   "source": [
    "Whith:\n",
    "- $\\Delta$ c the molar concentration change (in M)\n",
    "- $\\epsilon_\\lambda$ the moral absorption coefficient (in $M^-1 cm^-1$) for a given wavelength $\\lambda$\n",
    "- l the the optical pathlength \n",
    "- B the pathlength correction factor\n",
    "\n",
    "For more information on the equation, see the reference: Delpy DT, Cope M, Zee P van der, Arridge S, Wray S, Wyatt J. Estimation of optical pathlength through tissue from direct time of flight measurements. Phys Med Biol 1988; 33: 1433-1442."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f1a01",
   "metadata": {},
   "source": [
    "Run the cell below to compute the concentration changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5957ebae-707c-46b7-b342-41d81f33356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required beer-lambert law related function\n",
    "from mne.preprocessing.nirs import beer_lambert_law\n",
    "#Convert optical density into concentration change\n",
    "raw_haemo=beer_lambert_law(raw_od)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fc5db",
   "metadata": {},
   "source": [
    "#### Filtering out physiological oscillations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba952ae1-d0dc-4962-89bf-8a3aa842eea4",
   "metadata": {},
   "source": [
    "Let's now filter out physiological oscillations.\n",
    "\n",
    "To begin, we'll assess the extent of physiological oscillations within the data using power spectral density (PSD) analysis. Based on the sequence of events outlined previously, we note that stimuli for a same condition (e.g., tapping right) are presented at a rate of approximately once every 1/100 seconds. Consequently, we should anticipate a hemodynamic response to this specific stimulus occurring at this particular frequency. Any additional components identified in the PSD analysis can be attributed to other forms of oscillations, including physiological ones.\n",
    "\n",
    "Run the following cell to initiate the signal decomposition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = spectrum = raw_haemo.compute_psd().plot(average = True)\n",
    "fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54679-bb1a-4005-bb96-248cd96b2b8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The PSD shows a very low peak which may correspond to the hemodynamic responses to task stimulations. On the other hand, the presence of a frequency peak at 1.25 Hz aligns with the heart rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ac0a6-3c4b-4e03-b7ac-389dc8a01cb8",
   "metadata": {},
   "source": [
    "Based on the above information, apply a low-pass filter to only retain the hemodynamic responses. You may especially need to read the documentation on filters proposed by mne library (https://mne.tools/stable/generated/mne.filter.filter_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db449cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_haemofiltered = #Your answer here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67022520",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>💡 Pay attention! 💡</b></p>\n",
    "<p style='text-indent: 10px;'> Whenever you apply a filter, you should check that you are not removing the signal of interest. For that, make sure the task stimulation frequency is not within the frequency range of your filter ! </p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd00034-147c-46d0-80cc-c31d1e2a2332",
   "metadata": {},
   "source": [
    "Run the next cell to plot the PSD of your filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the hemoglobin concentration change\n",
    "fig = spectrum = raw_haemofiltered.compute_psd().plot(average = True)\n",
    "fig.suptitle('After filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff134ca",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the time-series of one channel\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "raw_haemofiltered.plot(start=120,duration=80,n_channels=56,scalings=dict(hb0=1e-7,hbr=1e-7),show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ff740-756b-44cc-8c8b-3074c0e5dc6c",
   "metadata": {},
   "source": [
    "Congratulations ! You now have all the basics to understand and preprocess fNIRS data!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136f23d",
   "metadata": {},
   "source": [
    "<h1><font color='black'>Part 2: diffusion data and tractography generation </font></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7e7b",
   "metadata": {},
   "source": [
    "The preprocessing in DTI involves similar steps to what you saw in fMRI. We will thus tackle specific different steps to not repeat ourselves too much:\n",
    "<img src=\"imgs/DTI_preprocessing.png\" />\n",
    "<p  style=\"text-align: center;\"><i>Image from <a href=\"https://www.researchgate.net/publication/311246309_Imaging_analysis_of_Parkinson's_disease_patients_using_SPECT_and_tractography\">Son, Seong-Jin, Mansu Kim, and Hyunjin Park. \"Imaging analysis of Parkinson’s disease patients using SPECT and tractography.\" Scientific reports 6.1 (2016): 1-11.</a></i></p>\n",
    "\n",
    "In other words, we will have you look at an example to generate tractogram. Here, a tractogram will be generated using a deterministic algorithm EuDX.  \n",
    "  \n",
    "Diffusion tensor imaging (DTI) is one of the most popular MRI techniques to describe the orientation of white matter fibers in brain research. The process of fiber tracking is called tractography. It allows for a virtual dissection and three-dimensional representation of white matter tracts. \n",
    "While we could still use FSL for the task, we will have you use <a href=\"https://dipy.org/\">DIPY</a>, a Python package for computational neuroanatomy mainly focusing on diffusion MRI analysis.  \n",
    "<br>\n",
    "To generate a tractogram, we need to track the fibers, which is called fiber tracking.\n",
    "<br>\n",
    "Local fiber tracking is used to model white matter fibers by creating streamlines from local directional information. In order to perform local fiber tracking, you will apply the following three steps:\n",
    "<p >\n",
    "    <ol>\n",
    "        <li style=\"font-size: 15px;\">Extract directions from diffusion data</li> \n",
    "        <li style=\"font-size: 15px;\">Identify when the tracking must stop</li>  \n",
    "        <li style=\"font-size: 15px;\">Select a set of locations from which to begin tracking</li>\n",
    "    </ol>\n",
    "</p>\n",
    "Combining them will help you obtain a tractography reconstruction!\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdbd89",
   "metadata": {},
   "source": [
    "## 0. An additional package\n",
    "We require two additional packages for this lab to work! They are only for visualization.\n",
    "To install them, you will need to use the following commands:\n",
    "```\n",
    "conda install -c conda-forge fury\n",
    "conda install -c conda-forge ipyvtklink \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582b7ba",
   "metadata": {},
   "source": [
    "### 1. Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.io.image import load_nifti, load_nifti_data\n",
    "\n",
    "hardi_fname, hardi_bval_fname, hardi_bvec_fname = get_fnames('stanford_hardi')\n",
    "label_fname = get_fnames('stanford_labels')\n",
    "\n",
    "data, affine, hardi_img = load_nifti(hardi_fname, return_img=True)\n",
    "labels = load_nifti_data(label_fname)\n",
    "bvals, bvecs = read_bvals_bvecs(hardi_bval_fname, hardi_bvec_fname)\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data.shape: ',data.shape)\n",
    "print('affine.shape: ',affine.shape)\n",
    "print('hardi_img.shape: ',hardi_img.shape)\n",
    "\n",
    "print('labels.shape: ',labels.shape)\n",
    "print('bvals.shape: ',bvals.shape)\n",
    "print('bvecs.shape: ',bvecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d842124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "print_dir_tree(op.split(hardi_fname)[0], max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cc3fe",
   "metadata": {},
   "source": [
    "### 2. Get the directions from the diffusion data set\n",
    "\n",
    "#### 1. Defining the white matter region.\n",
    "Before all else, you will need to visualize the labels above. Run the following cell to load the labels on FSLeyes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d852ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(label_fname)\n",
    "displayCtx.getOpts(overlayList[0]).cmap = 'brain_colours_spectrum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f992f",
   "metadata": {},
   "source": [
    "Based on the values you read within, can you please fill in the cell below with the label corresponding to white matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_matter_value = ??? # Fill with the value you read in FSLeyes for white matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493caf5f",
   "metadata": {},
   "source": [
    "Great, let's now visualize the result of the mask, shall we? For this, let's generate the mask we obtained from above, using our best pal fslmaths ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_name = 'extracted_wm'\n",
    "cmd = 'fslmaths ' + label_fname + ' -thr ' + str(white_matter_value) + ' -uthr ' + str(white_matter_value) + ' -bin ' + output_name\n",
    "os.system(cmd)\n",
    "load(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948b411",
   "metadata": {},
   "source": [
    "That's nice, but we are missing something, aren't we? Look in the middle, there is a clear gap in purple. Why is that? If you have on top, you'll see it has a different label: 2. This is because this region is white matter, but it is also a sagittal slice of the **corpus callosum**. So we need to do something a bit different. Can you think of a way to modify the above fslmaths command to include both the white matter and the slice of corpus callosum ? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_threshold = ??? # Select the lower bound to include both white matter and corpus callosum slice\n",
    "upper_threshold = ?? # Select the upper bound to include both white matter and corpus callosum slice\n",
    "output_name = 'extracted_wm_complete'\n",
    "cmd = 'fslmaths ' + label_fname + ' -thr ' + str(lower_threshold) + ' -uthr ' + str(upper_threshold) + ' -bin ' + output_name\n",
    "os.system(cmd)\n",
    "load(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7d1f1",
   "metadata": {},
   "source": [
    "Beautiful! So you can see that these labels can be a bit tricky if you're not careful. Based on your above experience above, we will construct a mask in python directly. Please fill in the cell below the two values for:\n",
    "- The white matter regions\n",
    "- The corpus callosum slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b54436",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_callosum_slice_value = ??? # Fill with your value!\n",
    "white_matter_value = ??? # Fill with your value !\n",
    "\n",
    "total_white_matter = (labels == corpus_callosum_slice_value) | (labels == white_matter_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068803fa",
   "metadata": {},
   "source": [
    "#### 2. Actually extracting fiber orientations: the orientation distribution function\n",
    "Okay, now we have a mask to define our fibers. The next cell will be used to estimate the orientation distribution function at each voxel. Before going any further, let's ask why this is necessary. In your opinion, in a single *voxel* how many orientations can we have?\n",
    "- [ ] Exactly one, since only one fiber is passing through the voxel\n",
    "- [ ] One, as the orientation describes the voxel's orientation, not the fibers going through the voxel\n",
    "- [ ] 26, since there are 26 neighbouring voxels with which a link is possible\n",
    "- [ ] As many orientations as there are fibers going through the voxel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a549a23",
   "metadata": {},
   "source": [
    "The issue can be summarized as resolving **intravoxel** fiber orientations of MR images.\n",
    "To summarize these, we use an orientation distribution function, coined ODF.\n",
    "\n",
    "<div class=\\\"warning\\\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "    <span>\n",
    "    <p style='margin-top:1em; text-align:center'><b>💡 Pay attention! 💡</b></p>\n",
    "    <p style='text-indent: 10px;'>\n",
    "        ODF really stands for orientation distribution function here, <b>not</b> ordinary differential function or anything else. Don't confuse the two!</p>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "We will not bore you with all mathematical details. What you need to know, however, is that this distribution function will rely on a special model, called the constant solid angle ODF model. The idea is the following: considering the distance from origin of the estimated distribution provides useful information. Here's what the solid angle looks like:\n",
    "<img src=\"imgs/tractography/tileshop.jpeg\"/>\n",
    "<center>Left: pdf takes into account solid angle; Right: pdf does not take into account solid angle</center>\n",
    "<i><center>Image taken from Aganj, Iman, et al. \"Reconstruction of the orientation distribution function in single‐and multiple‐shell q‐ball imaging within constant solid angle.\" Magnetic resonance in medicine 64.2 (2010): 554-566.</center></i>\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0241fda",
   "metadata": {},
   "source": [
    "Let's now estimate the orientation distribution function of each voxel, using the CSA-ODF model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import auto_response_ssst\n",
    "from dipy.reconst.shm import CsaOdfModel\n",
    "from dipy.data import default_sphere\n",
    "from dipy.direction import peaks_from_model\n",
    "\n",
    "# Single fiber response function: the measured signal of a single fiber\n",
    "# sume: regions where there are single coherent fiber populations\n",
    "# auto_response_ssst: calculate FA for a ROI of radii equal to roi_radii in the center of the volume\n",
    "# and return the response function estimated in that region for the voxels with FA higher than 0.7\n",
    "response, ratio = auto_response_ssst(gtab, data, roi_radii=10, fa_thr=0.7)\n",
    "\n",
    "# Instantiate the Constant Solid Angle model\n",
    "csa_model = CsaOdfModel(gtab, sh_order=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b205286",
   "metadata": {},
   "source": [
    "Now that we have our model, the orientation of tract segments can be extracted, looking at the peaks in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "csa_peaks = peaks_from_model(csa_model, data, default_sphere,\n",
    "                             relative_peak_threshold=.8,\n",
    "                             min_separation_angle=45,\n",
    "                             mask=total_white_matter, npeaks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041ed92",
   "metadata": {},
   "source": [
    "Notice in the above cell the following line:\n",
    "```python\n",
    "csa_peaks = peaks_from_model(..., npeaks=5)\n",
    "```\n",
    "\n",
    "This means that really, we extract per voxel five peaks at most. This is an important assumption. Depending on your voxel size, you might want to pay attention to this number!\n",
    "\n",
    "To confirm this, let's have a look at the extracted peak values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "csa_peaks.peak_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef696642",
   "metadata": {},
   "source": [
    "Knowing the MR dimensions, you can see that we indeed have five peaks per voxel. Great! Let's visualize it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695b081",
   "metadata": {},
   "source": [
    "## Checkpoint: install an additional library\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <p>Please go on the terminal and install the library <b>fury</b> by running:</p>\n",
    "        \n",
    "        conda install -c conda-forge fury\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16fb20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i>Make sure you are on the usual environment!</i>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fury import actor, window, ui\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = window.Scene()\n",
    "slice_actor = actor.peak_slicer(csa_peaks.peak_dirs,\n",
    "                            csa_peaks.peak_values,\n",
    "                            affine=affine,mask=total_white_matter,\n",
    "                            colors=None)\n",
    "scene.add(slice_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e1250",
   "metadata": {},
   "source": [
    "So as you can see, the orientations do map out to the expected directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8dd87",
   "metadata": {},
   "source": [
    "### 3. Set the stop criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77906254",
   "metadata": {},
   "source": [
    "Now, we need to setup our fiber tracking to stop it. What criterion should we use?\n",
    "Well, we'll roughly use the idea that when we don't have enough evidence to know where a fiber could have gone, we stop tracking it.\n",
    "In other words, if there are areas where the diffusion is totally unrestricted (goes in all directions), we have no clue as to where the fiber might continue. For this, we can threshold the tendency of our peaks to depend on a specific direction (anisotropy).<br>\n",
    "More specifically, we will threshold the general fractional anisotropy of our data to decide when we should stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc23864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion\n",
    "\n",
    "stopping_criterion = ThresholdStoppingCriterion(csa_peaks.gfa, .25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b904f4b",
   "metadata": {},
   "source": [
    "Let's visualize a slice! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5603d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sli = csa_peaks.gfa.shape[2] // 2\n",
    "plt.figure('GFA')\n",
    "plt.subplot(1, 2, 1).set_axis_off()\n",
    "plt.imshow(csa_peaks.gfa[:, :, sli].T, cmap='gray', origin='lower')\n",
    "\n",
    "plt.subplot(1, 2, 2).set_axis_off()\n",
    "plt.imshow((csa_peaks.gfa[:, :, sli] > 0.25).T, cmap='gray', origin='lower')\n",
    "\n",
    "plt.savefig('gfa_tracking_mask.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5117d0",
   "metadata": {},
   "source": [
    "### 4. Specify where to begin the fibers tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101ba67",
   "metadata": {},
   "source": [
    "There are different ways to place seeds, ie starting points from which the fiber tracking is started. This depends on the pathways you might like to model! For example, if you only wanted to model the corpus callosum it would not be so interesting to place seeds in other regions of the brain. <br>\n",
    "So that you understand what the output of the cell below will be, we must first explain what you'll extract in the cell below.<br>\n",
    "The orientation of an image is described by its affine transformation, if you remember well. Let's call this affine $A$.\n",
    "A seed point at the center of voxel $[i,j,k]$ will be represented as $[x,y,z, 1]= A \\cdot [i,j,k,1]$<br>\n",
    "In other words, you will get **coordinates in voxel space**. Note that there is one important assumption: the voxels here should be isotropic (the size of a voxel should be same along all directions).\n",
    "\n",
    "In our specific case, we will start from a sagittal slice of the **corpus callosum**, the one with label 2 to be specific.\n",
    "Please, create the mask (based on the labels above) to extract **only the slice of corpus callosum with label 2 as a mask**. You can refer to what we did above to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "\n",
    "seed_mask = ??? # Your code here to extract only the place of interest! \n",
    "seeds = utils.seeds_from_mask(seed_mask, affine, density=[2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277de50",
   "metadata": {},
   "source": [
    "### 5. Bringing it all together and generating the streamlines\n",
    "Let's see our ingredients:\n",
    "- Seeds, generated above and starting from a slice of the corpus callosum\n",
    "- A mask of regions where we should stop our fibers, based on anisotropy\n",
    "- Peaks of ODF, at most five peaks per voxel\n",
    "\n",
    "It remains now to combine all of these to bake so called streamlines (ie: fibers!). To do so, we will use the EuDX algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "\n",
    "# Initialization of LocalTracking. The computation happens in the next step.\n",
    "streamlines_generator = LocalTracking(csa_peaks, stopping_criterion, seeds,\n",
    "                                      affine=affine, step_size=.5)\n",
    "# Generate streamlines object\n",
    "streamlines_t = Streamlines(streamlines_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07184cd0",
   "metadata": {},
   "source": [
    "Beautiful! Let's now visualize our streamlines!\n",
    "Remember that they represent **only lines that start from the corpus callosum**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da572e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import colormap\n",
    "\n",
    "# Prepare the display objects.\n",
    "color = colormap.line_colors(streamlines_t)\n",
    "\n",
    "streamlines_actor = actor.line(streamlines_t,\n",
    "                               colormap.line_colors(streamlines_t))\n",
    "\n",
    "# Create the 3D display.\n",
    "scene = window.Scene()\n",
    "scene.add(streamlines_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())\n",
    "\n",
    "# Save still images for this static example. Or for interactivity use\n",
    "#window.record(scene, out_path='tractogram_EuDX.png', size=(800, 800))\n",
    "#if interactive:\n",
    "#    window.show(scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05149fea",
   "metadata": {},
   "source": [
    "### 6. Store the streamlines into a trackvis file\n",
    "\n",
    "What if we wanted to save the result as a file? Well, you can! For this, we need to save it to a special format, the TrackVis (.trk) format.\n",
    "\n",
    "Remember: our goal was to generate the streamlines. It is these streamlines that we therefore want to save! :) \n",
    "Let's do it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f052b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
    "from dipy.io.streamline import save_tractogram, save_trk\n",
    "\n",
    "# This is for the cc slice tractogram\n",
    "sft = StatefulTractogram(streamlines_t, hardi_img, Space.RASMM)\n",
    "save_trk(sft, \"tractogram_EuDX.trk\", streamlines_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61be9e0",
   "metadata": {},
   "source": [
    "If you want to visualize it all, you can install <a href=\"http://trackvis.org/download/\">TrackVis</a> and open the file from within! (TrackVis is free, no worries). It will be interactive and in 3D! Pretty cool huh?<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cc30f",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Let's sum up what we've seen. For a successfull tractography generation, we need the following: \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ingredient</th>\n",
    "        <th>Role</th>\n",
    "        <th>How is it created?</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Seeds</td>\n",
    "        <td>Define starting point of tract propagation.</td>\n",
    "        <td>Can be done randomly or according to some mask of interest</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Diffusion directions</td>\n",
    "        <td>Define the local diffusion in a voxel, for all voxels of interest</td>\n",
    "        <td>Can be done with CSA-ODF or other methods such as e.g structure tensor</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Stopping criteria</td>\n",
    "        <td>Defines where the tract continues or stops.</td>\n",
    "        <td>Can be done based on anatomy, information of diffusion direction, combination of both...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>A tracking algorithm</td>\n",
    "        <td>Combines all ingredients above to generate streamlines</td>\n",
    "        <td>Line propagation techniques to grow from seed region, or probabilistic with a pdf of fiber orientations.</td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "<img src=\"imgs/tractography/4qamin.jpg\"/>\n",
    "\n",
    "Each of the ingredients can be changed for a different flavour. You can explore <a href=\"https://dipy.org/tutorials/\">DIPY's tutorials</a> to get an idea of the changes you can operate. Feel free to play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ffc0a",
   "metadata": {},
   "source": [
    "# Additional: Connectivity analysis based on tractography "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0f481",
   "metadata": {},
   "source": [
    "By using the generated streamlines, we could analize the brain connectivity, for example, which streamlines pass through or not pass through some regions of the brain, how many streamlines are connecting two ROI, etc. To do this, it would be better if we could create a tractography with seeds spaning the entire white matter. Due to RAM concern, we will not do it here. But please feel free to explore it if you are interested! You will find some useful tutorials here:\n",
    "   https://dipy.org/documentation/1.7.0/examples_built/#streamlines-analysis-and-connectivity\n",
    "\n",
    "This ends this short tractography tutorial! Again, do not hesitate to explore more on DIPY's website if you're interested :)\n",
    "We strongly encourage you to use TrackVis for visualization of tractograms as it's really made for it and is much more intuitive to use for this purpose than FSLeyes.\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac02d6a-d896-48e0-a36c-e280f83b37df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p><b>Congratulations for having finished this part of the laboratory! </b></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
