{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0005f9d3-e329-4837-918e-9df0b384489e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a8a63-df82-4cae-b9d1-f28fcec4f71b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Welcome to the laboratory computers for the course \"Neural signals and signal processing\". \n",
    "This week, we finish up the preprocessing of fMRI on some advanced points, which you might need when working on your mini-project.\n",
    "We will then look at functional Near Infrared Spectroscopy (fNRIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b7844c-199f-423c-9336-eef343b02430",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%gui wx\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "# Required to get utils.py and access its functions\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist, interactive_MCQ,get_json_from_file\n",
    "\n",
    "####################\n",
    "# DIPY_HOME should be set prior to import of dipy to make sure all downloads point to the right folder\n",
    "####################\n",
    "os.environ[\"DIPY_HOME\"] = \"/home/jovyan/Data\"\n",
    "\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "###################\n",
    "# Load all relevant libraries for the lab\n",
    "##################\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "import ants\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dff04c6-e097-450c-8ae2-033f9cabb72b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "  .fit {\n",
       "    object-fit: cover;\n",
       "  }\n",
       "\n",
       "  .container {\n",
       "    display: flex;\n",
       "    align-items: center; /* Align blocks vertically in the middle */\n",
       "    justify-content: flex-start; /* Align blocks to the left */\n",
       "  }\n",
       "\n",
       "  .imageDiv {\n",
       "      background: #fff;\n",
       "      display: block;height: 150px;width: 150px;padding: 10px;border-radius: 2px;box-shadow: 0 1px 4px rgba(0, 0, 0, 0.3), 0 0 40px rgba(0, 0, 0, 0.1) inset;flex-shrink: 0;\n",
       "  }\n",
       "    .arrow-right {\n",
       "      width: 0; \n",
       "      height: 0; \n",
       "      border-top: 1em solid transparent;\n",
       "      border-bottom: 1em solid transparent;\n",
       "      border-left: 1em solid #000;\n",
       "    }\n",
       "      .col-md-10 {\n",
       "        display: flex;\n",
       "        align-items: center;\n",
       "      }\n",
       "    \n",
       "    .bby {\n",
       "        display: flex;\n",
       "        justify-content: center;\n",
       "        align-items: center;\n",
       "        height: 100vh;\n",
       "        font-family: Arial, sans-serif;\n",
       "    }\n",
       "    \n",
       "    .flow-container {\n",
       "        display: flex;\n",
       "        align-items: center;\n",
       "        justify-content:center;\n",
       "    }\n",
       "    \n",
       "    .box-text-container {\n",
       "        flex-direction: column;\n",
       "        display: center;\n",
       "        align-items: center;\n",
       "        justify-content:center;\n",
       "    }\n",
       "    \n",
       "    .step-box {\n",
       "        background-color: lightgray;\n",
       "        padding: 0.5em;\n",
       "        margin: 0 0.5em;\n",
       "        text-align: center;\n",
       "        font-weight: bold;\n",
       "        border-radius: 0.25em;\n",
       "        border: 0.15em solid black;\n",
       "        min-width: 6em;\n",
       "    }\n",
       "    \n",
       "    .arrow {\n",
       "        font-size: 1.5em;\n",
       "        color: blue;\n",
       "        font-weight: bold;\n",
       "        margin: 0 0 0.5em;\n",
       "    }\n",
       "    \n",
       "    .text {\n",
       "        font-weight: bold;\n",
       "        font-size: 0.9em;\n",
       "        margin: 0 0.5em;\n",
       "    }\n",
       "    \n",
       "    .box-wrapper {\n",
       "        display: flex;\n",
       "        align-items: center;\n",
       "        padding: 0.5em;\n",
       "        border: 0.15em solid black;\n",
       "        border-radius: 10px;\n",
       "    }\n",
       "    \n",
       "    /* Style for the text below the box */\n",
       "    .kapt_2 {\n",
       "        margin-top: 0.5em;\n",
       "        font-size: 1em;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "\n",
       "    .disp_p {\n",
       "        font-size:2.5em;\n",
       "    }\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Inject custom CSS\n",
    "custom_css =\"\"\"\n",
    "<style>\n",
    "\n",
    "  .fit {\n",
    "    object-fit: cover;\n",
    "  }\n",
    "\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: center; /* Align blocks vertically in the middle */\n",
    "    justify-content: flex-start; /* Align blocks to the left */\n",
    "  }\n",
    "\n",
    "  .imageDiv {\n",
    "      background: #fff;\n",
    "      display: block;height: 150px;width: 150px;padding: 10px;border-radius: 2px;box-shadow: 0 1px 4px rgba(0, 0, 0, 0.3), 0 0 40px rgba(0, 0, 0, 0.1) inset;flex-shrink: 0;\n",
    "  }\n",
    "    .arrow-right {\n",
    "      width: 0; \n",
    "      height: 0; \n",
    "      border-top: 1em solid transparent;\n",
    "      border-bottom: 1em solid transparent;\n",
    "      border-left: 1em solid #000;\n",
    "    }\n",
    "      .col-md-10 {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "      }\n",
    "    \n",
    "    .bby {\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        height: 100vh;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }\n",
    "    \n",
    "    .flow-container {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content:center;\n",
    "    }\n",
    "    \n",
    "    .box-text-container {\n",
    "        flex-direction: column;\n",
    "        display: center;\n",
    "        align-items: center;\n",
    "        justify-content:center;\n",
    "    }\n",
    "    \n",
    "    .step-box {\n",
    "        background-color: lightgray;\n",
    "        padding: 0.5em;\n",
    "        margin: 0 0.5em;\n",
    "        text-align: center;\n",
    "        font-weight: bold;\n",
    "        border-radius: 0.25em;\n",
    "        border: 0.15em solid black;\n",
    "        min-width: 6em;\n",
    "    }\n",
    "    \n",
    "    .arrow {\n",
    "        font-size: 1.5em;\n",
    "        color: blue;\n",
    "        font-weight: bold;\n",
    "        margin: 0 0 0.5em;\n",
    "    }\n",
    "    \n",
    "    .text {\n",
    "        font-weight: bold;\n",
    "        font-size: 0.9em;\n",
    "        margin: 0 0.5em;\n",
    "    }\n",
    "    \n",
    "    .box-wrapper {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        padding: 0.5em;\n",
    "        border: 0.15em solid black;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    \n",
    "    /* Style for the text below the box */\n",
    "    .kapt_2 {\n",
    "        margin-top: 0.5em;\n",
    "        font-size: 1em;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\n",
    "    .disp_p {\n",
    "        font-size:2.5em;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(custom_css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a45f4ca-3fff-4a7f-871f-2cc778b8a304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 21:29:09.313: Failed to load module \"canberra-gtk-module\"\n",
      "\n",
      "(ipykernel_launcher.py:14302): Gtk-CRITICAL **: 21:29:09.521: gtk_window_resize: assertion 'height > 0' failed\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n",
      "21:29:10: Debug: ScreenToClient cannot work when toplevel window is not shown\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Start FSLeyes (very neat tool to visualize MRI data of all sorts) within Python\n",
    "################\n",
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de61548-ef10-4f72-93f9-c252fca6aba9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 1: Advanced preprocessing (Optional)\n",
    "\n",
    "This part is a continuation of last week. It is optional, but some parts (such as \"Applying the transformation to all volumes\") might be helpful for your miniprojects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2caee9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.1 Field map unwarping\n",
    "\n",
    "The field itself is not homogeneous, as you've seen in class. This means, in turn, that there are distortions in the acquisition.\n",
    "We can try to correct for it, through field maps, provided they've been acquired.\n",
    "\n",
    "### What are field maps ? \n",
    "\n",
    "Field maps are maps of the magnetic field (hence their name). They are acquired during an experimental session to capture parts where the MRI's magnetic field might present inhomogeneities. These inhomogeneities will, in turn, cause distortions in the signal which are not part of the subject's anatomy, as well as signal drop (places where the contrast becomes very small between tissues). Such artefacts should obviously be removed.\n",
    "This is where fieldmaps are typically coming into play: by knowing how your scanner is distorting your signal, you can hope to correct for it - to some amount.\n",
    "\n",
    "The first step is - naturally - to acquire fieldmaps.\n",
    "\n",
    "Fortunately this is the case in our dataset - but you will need to download them as we have avoided loading them for you - on purpose!\n",
    "\n",
    "To make sure you've understood how to load datasets, here is the dataset of interest: https://openneuro.org/datasets/ds004226/versions/1.0.0\n",
    "\n",
    "<img src=\"imgs/dataset_screen.png\">\n",
    "\n",
    "Your first task is to load:\n",
    "- Subject 001 data files, including the fieldmap files, located in the fmap subfolder (WITH the JSON sidecars!)\n",
    "\n",
    "Out of convenience, we already provide you with the openneuro-py case. Modify the command-line run below to include *all files in the fmap subfolder of sub-01*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff35362-85d1-42f1-adfc-69a62c8bd5ce",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "dataset_id = 'ds004226'\n",
    "subject = '001'\n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(os.path.abspath(\"\"),sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "fmap_path = op.join(bids_root, 'sub-001', 'fmap')\n",
    "subject_dir = 'sub-{}'.format(subject)\n",
    "\n",
    "##################\n",
    "# Solution\n",
    "# There are two solutions\n",
    "# The easiest is simply to include all data of subject-01\n",
    "# The other is to add one line for the fieldmaps\n",
    "##################\n",
    "# Change the command below to include files in the fmap subdirectory\n",
    "# You should STILL be loading the EPI and anatomical\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", op.join(subject_dir, 'anat','*'),# We are asking to get all files within the subject_dir/anat folder by using the wildcard *\n",
    "                \"--include\", op.join(subject_dir, 'func','*'),# We are asking to get all files within the subject_dir/func folder by using the wildcard *\n",
    "                \"--include\", op.join(subject_dir, 'fmap','*'),# We are asking to get all files within the subject_dir/fmap folder by using the wildcard *\n",
    "               ], check=True)\n",
    "\n",
    "# Simple variant to include everything from subject-01\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", subject_dir # Effectively get all data\n",
    "               ], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25bcf5-efdd-4daa-85ae-00daacba454b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Again, remember this download might not be finished immediately :)\n",
    "Now, assuming it *is*, let's have a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7228ba-fba5-4a95-b493-4ea194e1d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85a3de-2a3e-4fe7-9b4c-a63e18ddbd59",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Look in the fmap folder. There are four files, corresponding to two fieldmap acquisitions. One is PA, the other is AP.\n",
    "\n",
    "We need some parameters to be able to exploit these files.\n",
    "In particular, we need to figure out:\n",
    "- The phase encoding direction\n",
    "- The total readout time\n",
    "\n",
    "Your task is to figure out which keys to exploit for this.\n",
    "Have a look at the code below (and feel free to play around a bit of course!) to setup the values properly.\n",
    "To help you, we've loaded one of the two JSON sidecars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d1a9b-0ed8-4c6d-ae2c-daac31b1724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_json_from_file(op.join(fmap_path, 'sub-001_acq-task_dir-{}_epi.json'.format('AP')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fe3b2-614d-4734-b93d-ea85eb757350",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_no_exist(deriv_root)\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'anat'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'fmap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219030dd-f854-47e9-9cae-c7d6be72cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_fmap_path = op.join(preproc_root, 'sub-001', 'fmap')\n",
    "mkdir_no_exist(preproc_fmap_path)\n",
    "direction_file = op.join(preproc_fmap_path, 'datain.txt')\n",
    "\n",
    "f = open(direction_file, 'w')\n",
    "\n",
    "for name in ['AP', 'PA']:\n",
    "    data = get_json_from_file(op.join(fmap_path, 'sub-001_acq-task_dir-{}_epi.json'.format(name)))\n",
    "    phase_dir = data['PhaseEncodingDirection'] # Extract here the phase encoding direction !\n",
    "    total_readout_time = data['TotalReadoutTime'] # Extract here the total readout time !\n",
    "    \n",
    "    # We expect a specific format, namely x y z total_readout_time, where x,y and z are set to 1/-1 if and only if they are the phase\n",
    "    # encoding direction, 0 otherwise.\n",
    "    phase = [0, 0, 0, total_readout_time]\n",
    "    is_neg = len(phase_dir) == 2 and phase_dir[1] == '-'\n",
    "    phase_dir = phase_dir[0]\n",
    "    phase[ord(phase_dir)-ord('i')] = -1 if is_neg else 1\n",
    "    for i in range(3):\n",
    "        f.write('{} {} {} {}\\n'.format(phase[0], phase[1], phase[2], phase[3]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db073c57-6575-4c59-94ff-4a1ecd8c318b",
   "metadata": {},
   "source": [
    "### 1.1.1 Creating the field map\n",
    "Now, we will create the field map.\n",
    "This process is tedious, sometimes hard to get right. First, let's look at the two fieldmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c19424-b0fa-4902-8f42-64cd4d15d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(fmap_path, 'sub-001_acq-task_dir-AP_epi.nii.gz'))\n",
    "fsleyesDisplay.load(op.join(fmap_path, 'sub-001_acq-task_dir-PA_epi.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33545c42-1d20-484c-abb8-5c45995bcd74",
   "metadata": {},
   "source": [
    "As you notice, they are quite different with respect to their distortions. This is because they used different phase encoding directions (Anterior -> Posterior and Posterior -> Anterior, hence AP and PA).\n",
    "\n",
    "Looking from these two encoding directions, we (or rather clever algorithm: <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/topup.html\">topup</a>) can build a complete map of the distortions on our EPI / fMRI file.\n",
    "\n",
    "Here is the signature of topup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fa0fe-d090-4b23-bf76-c9453b16bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"topup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7427f-7a17-48d6-969e-17446c7e5b48",
   "metadata": {},
   "source": [
    "Not very clear, is it? You need to, in fact, conduct several steps. \n",
    "\n",
    "- First, AP and PA fieldmaps should be made a single file to be fed to topup.\n",
    "- You also need to feed it the direction file we created above (which specifies encoding direction and readout time of our EPI).\n",
    "- From the computed field, we need to convert it to radians, and finally, we obtain both a phase information for the field - in radians - and a magnitude information.\n",
    "\n",
    "You are now ready to apply the fieldmap to correct distortions.\n",
    "\n",
    "The function below conducts all these steps for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13aa48-53c9-4fe8-bede-dc49d1ffccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fmap_AP_PA(direction_file):\n",
    "    \"\"\"\n",
    "    From an AP/PA pair of files, generate the corresponding fieldmap files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    direction_file: str\n",
    "        Path to a direction file (typically called datain.txt) indicating phase encoding direction, total \n",
    "        readout time and other relevant parameters for fmap computation\n",
    "    \"\"\"\n",
    "    merged_phase_imgs = op.join(preproc_fmap_path, 'sub-001_acq-task_dir-fmap_merged')\n",
    "    \n",
    "    # Step 1: Combine AP and PA as single file\n",
    "    subprocess.run(['fslmerge', '-t', merged_phase_imgs, \n",
    "                    op.join(fmap_path, 'sub-001_acq-task_dir-AP_epi.nii.gz'), \n",
    "                    op.join(fmap_path, 'sub-001_acq-task_dir-PA_epi.nii.gz')])\n",
    "    \n",
    "    # Step 2: Compute the fieldmap deformation with topup \n",
    "    # In this particular step, we feed in the direction file (ie the phase encoding direction, phase order etc, which we've saved above as direction file\n",
    "    output_fmap = op.join(preproc_fmap_path, 'fieldmap_ex')\n",
    "    unwarped_img = op.join(preproc_fmap_path, 'se_epi_unwarped')\n",
    "\n",
    "    subprocess.run(['topup', \n",
    "                    '--imain={}'.format(merged_phase_imgs), \n",
    "                    '--datain={}'.format(direction_file),\n",
    "                   '--config={}'.format('b02b0.cnf'),\n",
    "                   '--fout={}'.format(output_fmap),\n",
    "                   '--iout={}'.format(unwarped_img),\n",
    "                   '-v'])\n",
    "    \n",
    "    # Step 3: Convert fmap units to rads\n",
    "    subprocess.run(['fslmaths', output_fmap, '-mul', str(6.28), output_fmap + '_rads'])\n",
    "    #fslmaths(output_fmap).mul(6.28).run(output_fmap + '_rads')\n",
    "    \n",
    "    # Step 4: Create magnitude fmap\n",
    "    subprocess.run(['fslmaths', unwarped_img, '-Tmean', output_fmap + '_mag'])\n",
    "    #fslmaths(unwarped_img).Tmean().run(output_fmap + '_mag')\n",
    "    \n",
    "    # Extract fmap brain using bet\n",
    "    subprocess.run(['bet', output_fmap + '_mag',output_fmap + '_mag_brain', '-m', '-R'])\n",
    "generate_fmap_AP_PA(direction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e8dc6-723b-43c9-823b-3062a648acd0",
   "metadata": {},
   "source": [
    "It now remains to apply the fieldmap! \n",
    "\n",
    "To do so we will apply to the **first volume of our series to show you the result of distortion correction**.\n",
    "Your first task is thus to extract the first volume by modifying the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_trim = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.nii.gz')\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "extracted_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_vol_1')\n",
    "\n",
    "###########\n",
    "# Solution\n",
    "# Extracting the first volume means we keep a single volume. \n",
    "# This is exactly as we did last week for the reference EPI used in epi_reg,\n",
    "# but we take the volume number 0 instead of the middle one.\n",
    "###########\n",
    "\n",
    "# Select only the FIRST volume!\n",
    "start_vol = 0 # Where should we start? (First volume is 0, not 1 !)\n",
    "number_of_volumes = 1 # How many volumes should we keep?\n",
    "\n",
    "fslroi(file_to_trim, extracted_epi, str(start_vol), str(number_of_volumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(extracted_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093777b",
   "metadata": {},
   "source": [
    "Great! \n",
    "We are finally able to apply our fieldmap to the EPI. We can do so, using FUGUE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38373a9-a9b8-4884-84c8-7d28f1aa40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_path_rad = op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads')\n",
    "epi_result= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_vol_1_fmap')\n",
    "\n",
    "unwarpdir='y-'\n",
    "dwell_time= data['EffectiveEchoSpacing'] # Although FUGUE wants the dwell time, the reported dwell time in the header is wrong: it should be in order of milliseconds (0.3 to 60ms is reasonable), but here it is on the order of microseconds. We use effective echo spacing instead, which is the same thing...And in correct order of magnitude.\n",
    "\n",
    "subprocess.run(['fugue', '-i', extracted_epi, # Fieldmaps are applied to EPI\n",
    "                '--loadfmap={}'.format(fmap_path_rad), # We used the fmap we just created\n",
    "               '--dwell={}'.format(dwell_time), # The dwell time is necessary as input parameter\n",
    "               '--unwarpdir={}'.format(unwarpdir), # The unwarp direction\n",
    "               '-u', epi_result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ff392-9223-4aac-8ee8-c13a5042fab2",
   "metadata": {},
   "source": [
    "Let's visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98f5f2-a3f1-4757-8bf7-3c0bd5eb3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(epi_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b876e-a801-4971-8b16-efe7fccc0311",
   "metadata": {},
   "source": [
    "You should observe the following two volumes:\n",
    "\n",
    "<center>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/uncorrected_brain.png\" style=\"height: 200px;width:auto;border: blue 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">Before unwarp (sagittal)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img class=\"middle-img\" src=\"imgs/corrected_brain.png\"/ style=\"height: 200px;width:auto;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">After unwarp (sagittal)</p>\n",
    "</div>\n",
    "<br><br>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/uncorrected_brain_2.png\" style=\"height: 270px;width:250px;border: blue 6px groove;\"/>\n",
    "    <p style=\"text-align:center;\">Before unwarp (axial)</p>\n",
    "</div>\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/corrected_brain_2.png\" style=\"height: 270px;width:250px;border: green 6px groove;\" />\n",
    "    <p style=\"text-align:center;\">After unwarp (axial)</p>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287d605-d9ec-405b-89f1-d064c2a432c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199cde7-6655-4b49-a845-1524e852cd2b",
   "metadata": {},
   "source": [
    "Do you think the fieldmap made an improvement? To drive your answer, feel free to inspect both volumes in FSLeyes. Observe the frontal and ventral regions. Do you notice anything different? Which one seems to match better what you'd expect from the brain anatomy ? \n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> Assessing quality of functional data 💡</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    When in doubt about what you observe, don't be afraid to go have a look at your T1 to compare against. If a structure in the functional shows up in the T1 but distorted, you'll find out faster this way.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d00452-9140-4482-9796-81e9feb03699",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>💡 Pay attention! 💡</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Easy? Well, not always. Field maps for this dataset came in a specific format. But they can come in <b>many</b> different ways, meaning you will need to be very careful when recovering them. The steps outlined above in particular are only applicable in the case of having an AP-PA acquisition. Here is the full resource of FSL's FUGUE on field map unwarping: <a href=https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fugue>https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fugue</a> . Don't be afraid to refer to it, should you have a different format in a project!</p>\n",
    "</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eedaaf",
   "metadata": {},
   "source": [
    "## 1.2 Combining transforms\n",
    "\n",
    "We know how to perform motion-correction, and how to coregister an image to another. That's great!\n",
    "But each transformation usually implies a step of interpolation (because the image is transformed and must be resampled). This interpolation means the resulting data is \"corrupted\" slightly. We would like to minimize the amount of interpolations to only once if possible.\n",
    "\n",
    "Here is a reminder of the different spaces we've considered, as well as the different operations and how they move between spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de1af2-539c-4a81-884f-b445c44790f0",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/epi_transform_steps.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766aaea-a775-4b18-a0a7-ea24ba690581",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50fc54-80c6-4aa6-9605-5fae3fa539b5",
   "metadata": {},
   "source": [
    "The order of transformations we would like to have is:\n",
    "\n",
    "<body>\n",
    "    <div class=\"flow-container\">\n",
    "        <div class=\"text\">EPI</div>\n",
    "        <div class=\"arrow\">→</div>\n",
    "        <div class=\"step-box\">Motion correction<br>Mcflirt</div>\n",
    "        <div class=\"arrow\">→</div>\n",
    "        <div class=\"text\">EPI<br> (motion corrected)</div>\n",
    "        <div class=\"arrow\">→</div>\n",
    "        <div class=\"step-box\">Field unwarping + affine coregistration<br>epi_reg with fieldmap</div>\n",
    "        <div class=\"arrow\">→</div>\n",
    "        <div class=\"text\">EPI<br> (Anatomical space)</div>\n",
    "        <div class=\"arrow\">→</div>\n",
    "        <div class=\"step-box\">Normalization<br>Flirt or ANTs</div>\n",
    "        <div class=\"arrow\">→</div>\n",
    "        <div class=\"text\">EPI<br> (template space)</div>\n",
    "    </div>\n",
    "\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dea5f",
   "metadata": {},
   "source": [
    "### 1.2.1 Combining fieldmap unwarping and EPI registration\n",
    "\n",
    "FSL provides a way to compute the EPI to anatomical while combining it with the fieldmap unwarping. We will show you in this part how to do it.\n",
    "\n",
    "But before applying all transforms, let's worry about doing the required preprocessing, namely:\n",
    "- Motion correction\n",
    "- Field unwarping\n",
    "- EPI to anatomical coregistration\n",
    "- Anatomical to template coregistration (Normalization): we will use the MNI152 1mm template that you know from the previous weeks\n",
    "\n",
    "\n",
    "First, remark that motion correction is done by selecting a reference volume in the EPI to which all others are coregistered. By default, the middle EPI was used. \n",
    "\n",
    "Because we used in our fieldmap computation the first EPI, we need to use this one instead.\n",
    "\n",
    "**It is critical that you pay attention to which image was used to compute your transformations, otherwise combining them won't make sense!**.\n",
    "\n",
    "For this reason, let's now go over the entire pipeline and transformation steps, sticking to the first EPI. We extract it again with fslroi, and we re-run the motion correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_epi = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "reference_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_first-vol')\n",
    "fslroi(original_epi, reference_epi, str(0), str(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869a2dd",
   "metadata": {},
   "source": [
    "Now, let's do motion correction. Recall that it is done on the **entire** EPI timeseries with mcflirt. We will explicitly give the first epi as reference this time around, to force FSL to use this volume and realign everyone to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "path_moco_data = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco')\n",
    "mcflirt(infile=original_epi,o=path_moco_data, plots=True, report=True, dof=6, mats=True, reffile=reference_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34053a6",
   "metadata": {},
   "source": [
    "Fantastic! Now, because **the reference volume did not move at all** (since it is the reference to which everyone is realigned), we can use this volume as starting point to compute our other transforms: we're only missing the coregistration with fieldmap unwarping, as the normalization is obtained through the anatomical data :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = '001'\n",
    "subject='sub-001'\n",
    "\n",
    "# Relevant paths for anatomical preprocessing\n",
    "anatomical_path = op.join(bids_root, subject, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id))\n",
    "betted_brain_path = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w'.format(subject_id))\n",
    "segmentation_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast')\n",
    "\n",
    "mni_template = op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm_brain'))\n",
    "anat_result = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w_mni'.format(subject_id))\n",
    "anat_2_mni_trans = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w_2_mni_lin.mat'.format(subject_id))\n",
    "\n",
    "# Relevant variables for epi_reg\n",
    "output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_anat-space')\n",
    "dwell_time = 0.000620007\n",
    "unwarpdir='y-'\n",
    "\n",
    "##############\n",
    "# Put here all steps which you might need to conduct before you can do epi_reg\n",
    "# Hint: consider the anatomical preprocessing steps, look at week 2 if you forgot!\n",
    "# For flirt, have a look at the omat argument to save the transform\n",
    "##############\n",
    "\n",
    "##############\n",
    "# Solution\n",
    "# We need to do the brain extraction, using bet as well as the segmentation, using fast, lastly we perform normalization to MNI152 space\n",
    "##############\n",
    "from fsl.wrappers import fast\n",
    "\n",
    "subprocess.run(['bet', anatomical_path, betted_brain_path, '-m', '-R'])\n",
    "\n",
    "fast(imgs=[betted_brain_path], out=segmentation_path, n_classes=3)\n",
    "\n",
    "flirt(betted_brain_path, mni_template, out=anat_result, omat = anat_2_mni_trans)\n",
    "\n",
    "#############\n",
    "# Launching epi_reg with fieldmap unwarping.\n",
    "# Careful to do it ON THE FIELDMAP CORRECTED VOLUME\n",
    "# Note epi_reg will take a few minutes to compute the transform - feel free to bombard us with questions (or candy)\n",
    "#############\n",
    "reference_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_first-vol')\n",
    "fslroi(original_epi, reference_epi, str(0), str(1))\n",
    "subprocess.run(['epi_reg','--epi={}'.format(reference_epi), \n",
    "                '--t1={}'.format(anatomical_path), \n",
    "                '--t1brain={}'.format(betted_brain_path), \n",
    "                '--out={}'.format(output_path),\n",
    "                '--fmap={}'.format(op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_rads')),\n",
    "                '--fmapmagbrain={}'.format(op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag_brain')),\n",
    "                '--fmapmag={}'.format(op.join(preproc_root, 'sub-001', 'fmap', 'fieldmap_ex_mag')),\n",
    "                '--wmseg={}'.format(op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast_pve_2')),\n",
    "                '--echospacing={}'.format(dwell_time),\n",
    "                '--pedir={}'.format(unwarpdir)])\n",
    "\n",
    "print(\"Done with EPI to anatomical registration with fieldmap unwarping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb3f1f",
   "metadata": {},
   "source": [
    "Beautiful! \n",
    "\n",
    "➡️ Inspect the two resulting files to ensure that nothing went wrong. In other words:\n",
    "- Check that the MCFLIRT result is okay with respect to motion\n",
    "- Check that the realignment following epi_reg made the EPI well aligned with the anatomical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa418-1fbe-4956-bfb5-808d3d601a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect here with fsleyes, such as fsleyesDisplay.load(yourEpi) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6197ec6-2059-48cc-9805-1da6a1e81c07",
   "metadata": {},
   "source": [
    "### 1.2.2 Getting the saved transformation\n",
    "\n",
    "Applying the transformation to a single volume is nice, but we should still need to know where the transformation was saved, to apply it to all other volumes of interest.\n",
    "\n",
    "Let's inspect our resulting folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e1820-c0d1-4833-aa90-0fce590f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root,max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babf6c7",
   "metadata": {},
   "source": [
    "The file <b>sub-001_task-sitrep_run-01_bold_anat-space_warp</b> corresponds to the <u>transformation</u> from EPI to anatomical file with the fieldmap unwarping applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287459ce-5f1f-4c43-89c4-a838790fb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6726a1f",
   "metadata": {},
   "source": [
    "### 1.2.3 Combining the transforms\n",
    "\n",
    "Let's list our available transforms as is:\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>Transform</th>\n",
    "    <th>Step</th>\n",
    "    <th>File(s)</th>\n",
    "    <th>Linear?</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Each EPI to reference EPI</td>\n",
    "    <td>Motion correction (mcflirt) </td>\n",
    "    <td>matrices in func/sub-001_task-sitrep_run-01_bold_moco.mat/</td>\n",
    "    <td>Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>EPI to anat (with fieldmap)</td>\n",
    "    <td>Functional to anat coregistration (epi_reg) </td>\n",
    "    <td>func/sub-001_task-sitrep_run-01_bold_anat-space_warp.nii.gz</td>\n",
    "    <td>No</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Anat to template</td>\n",
    "    <td>Normalization (FLIRT or FNIRT or ANTs)</td>\n",
    "    <td>anat/.mat</td>\n",
    "    <td>Yes if FLIRT or ANTs with linear transform, no otherwise</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "We will now combine all these transformations, so that we apply one transformation and exactly one interpolation at the end to minimize errors.\n",
    "\n",
    "We provide you with a function to do so just below, along with a function to apply the resulting combined transformation. Beware, all these functions operate on single volumes, not on 4D data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_transforms(reference_volume, warp_save_name,  is_linear, epi_2_moco=None, epi_2_anat_warp=None, anat_2_standard_warp=None):\n",
    "    \"\"\"\n",
    "    Combines transformation BEFORE motion correction all the way to standard space transformation\n",
    "    The various transformation steps are optional. As such, the final warp to compute is based on \n",
    "    which transforms are provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_volume: str\n",
    "        Reference volume. The end volume after all transformations have been applied, relevant for final resolution and field of view.\n",
    "    warp_save_name: str\n",
    "        Under which name to save the total warp\n",
    "    is_linear: bool\n",
    "        Whether the transformation is linear or non linear.\n",
    "    epi_2_moco: str\n",
    "        Transformation of the EPI volume to motion-correct it (located in the .mat/ folder of the EPI\n",
    "    epi_2_anat_warp: str\n",
    "        Transformation of the EPI volume to the anatomical space, typically obtained by epi_reg. Assumed to include fieldmap correction and thus be non-linear.\n",
    "    anat_2_standard_warp: str\n",
    "        Transformation of the anatomical volume to standard space, such as the MNI152 space. Might be linear or non linear, which affects is_linear value accordingly.\n",
    "    \"\"\"\n",
    "    from fsl.wrappers import convertwarp\n",
    "    args_base = {'premat': epi_2_moco, 'warp1': epi_2_anat_warp}\n",
    "    if is_linear:\n",
    "        args_base['postmat'] = anat_2_standard_warp\n",
    "    else:\n",
    "        args_base['warp2'] = anat_2_standard_warp\n",
    "    args_filtered = {k: v for k, v in args_base.items() if v is not None}\n",
    "\n",
    "    convertwarp(warp_save_name, reference_volume, **args_filtered)\n",
    "    print(\"Done with warp conversion\")\n",
    "\n",
    "def apply_transform(reference_volume, target_volume, output_name, transform):\n",
    "    \"\"\"\n",
    "    Applies a warp field to a target volume and resamples to the space of the reference volume.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_volume: str\n",
    "        The reference volume for the final interpolation, resampling and POV setting\n",
    "    target_volume: str\n",
    "        The target volume to which the warp should be applied\n",
    "    output_name: str\n",
    "        The filename under which to save the new transformed image\n",
    "    transform: str\n",
    "        The filename of the warp (assumed to be a .nii.gz file)\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    combine_all_transforms to see how to build a warp field\n",
    "    \"\"\"\n",
    "    from fsl.wrappers import applywarp\n",
    "    applywarp(target_volume,reference_volume, output_name, w=transform, rel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8860d23",
   "metadata": {},
   "source": [
    "Using these two functions should not be too hard now. Notice that in combine_all_transforms, setting any transform to None instead of the correct transform will skip the transform step in the total transformation. This way, you should be able to perform quality control. In particular, please ensure that:\n",
    "- [ ] Applying ONLY motion correction transformation to the first volume yields the expected alignement (so it should be aligned with the \\_moco volume.)\n",
    "- [ ] Applying motion correction + epi -> anat should be aligned to anatomical\n",
    "- [ ] Finally, applying motion correction + epi > anat + anat > standard should be aligned to the standard\n",
    "Only once you're convinced these steps are working well should you proceed to standard space. **Remember the 1-10-100 rule! Always perform QC before moving on.**\n",
    "\n",
    "To help you, we provide you below with the template to do such a thing, so that you don't have to worry too much about the nitty gritty details.\n",
    "Focus on:\n",
    "- The reference file to use \n",
    "- The transformations to provide (either a file or None)\n",
    "\n",
    "<b>Notice this is performed only on a single volume. Indeed, if you are debugging you should avoid wasting time applying transformations on entire timeseries to quickly diagnose whether a step is working or failing.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fsl.wrappers import applywarp\n",
    "ref=mni_template\n",
    "\n",
    "# We show this one when selecting the first EPI (volume 0000)\n",
    "target_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_first-vol')\n",
    "split_nbr = '0000'\n",
    "\n",
    "# We will name its warp as split0000\n",
    "warp_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_split' + split_nbr + '_epi_2_std_warp')\n",
    "\n",
    "# Get the transformation matrix of this volume (this one is actually the unit matrix, \n",
    "# since this volume is the reference)\n",
    "\n",
    "\n",
    "# -- Step 1: Combine the transformations, that is :\n",
    "#    EPI -> Motion correction -> Coregistration to anatomical -> Normalization to standard\n",
    "#    EPI -> Motion correction is given by the matrix in sub-001_task-sitrep_run-01_bold_moco.mat/MAT_{vol_nbr}, where {vol_nbr} is the volume number of the volume of interest\n",
    "#    EPI -> Coreg to anatomical, this is the _warp.nii.gz file in func/ folder\n",
    "#    Anatomical > Template is saved by flirt when doing the anatomical to template coregistration, in anat/ folder\n",
    "func_2_anat= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_anat-space_warp.nii.gz')\n",
    "epi_moco = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "\n",
    "s0 = time.time()\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "s1 = time.time()\n",
    "\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr)\n",
    "\n",
    "# -- Step 2: Apply the transformation to our EPI\n",
    "applywarp(target_epi,ref, out_vol, w=warp_name, rel=False)\n",
    "s2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ae196",
   "metadata": {},
   "source": [
    "Above, we've timed the steps to estimate which one might be more expensive, between combining the transforms and applying them. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transform combination time:', s1 - s0)\n",
    "print('Apply transform time:', s2 - s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac047cf",
   "metadata": {},
   "source": [
    "As you can clearly see, combining the transforms is more than 6 times slower than applying the final transform. As a consequence, we would like to do this step as rarely as we can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a00a9",
   "metadata": {},
   "source": [
    "#### 1.2.3.1 Optimizing a bit\n",
    "\n",
    "Okay, so this step is slow. Can we make it faster? Well, yes!\n",
    "\n",
    "Note that computing all these non linear fields <u>will</u> take time. We've seen above in fact that it is <u>the</u> most expensive step.\n",
    "Now, applywarp has a neat option. It allows us to apply a transformation using a pre transformation matrix followed by the warp. Why is it cool?\n",
    "Well, consider the following steps:\n",
    "\n",
    "<center><img src=\"imgs/space_steps.png\"></center>\n",
    "\n",
    "<br>\n",
    "Let's group transforms in two potential ways:\n",
    "\n",
    "<center><img src=\"imgs/two_ways_grouping.png\" style=\"max-width:1200px;\"></center>\n",
    "In other words, we consider <b>grouping up the transformations to apply them</b>. As you will see, computing transformations can take time when they are non linear, whereas applying them is comparatively fast. We will investigate both grouping all transforms together or grouping all transforms which follow motion correction together.\n",
    "\n",
    "Let's compare the two methods, runtime wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- START OF METHOD 1 \n",
    "# In this method, we compute the transform from start to finish and apply it\n",
    "s0 = time.time()\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "s1 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v1')\n",
    "#applywarp(target_epi,ref, out_vol, w=warp_name, rel=False)\n",
    "subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs'])\n",
    "s2 = time.time()\n",
    "# ----- START OF METHOD 2\n",
    "# In this method, we compute the transform only post motion correction. We apply the motion correction and then the warp\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "s3 = time.time()\n",
    "out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v2')\n",
    "subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs', '--premat={}'.format(epi_moco)])\n",
    "s4 = time.time()\n",
    "\n",
    "print('Method 1 runtime:', s2 - s0, '({} for combination, {} to apply)'.format(s1 - s0, s2 - s1))\n",
    "print('Method 2 runtime:', s4 - s2, '({} for combination, {} to apply)'.format(s3 - s2, s4 - s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9c4ef",
   "metadata": {},
   "source": [
    "To convince you that the two produced images are almost identical (you might notice differences on the order of the $10^{-3}$, but consider the relative error this entails and why such an error might happen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(ref)\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v1'))\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr + '_v2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67fe1d",
   "metadata": {},
   "source": [
    "Why does it matter? Well, just applying a back-of the envelope calculation, the first method takes 122s per volume, while the second method takes 87 seconds to combine **once** the transforms excluding motion correction, and 4 seconds per volume to apply the transforms including motion correction. If we plot the two with an increasing number of volumes, we can see why this quickly becomes relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 1000, 10)\n",
    "plt.plot(x, x*122, label='Method 1')\n",
    "plt.plot(x, 87 + x*4, label='Method 2')\n",
    "plt.xlabel('Number of volumes')\n",
    "plt.ylabel('Runtime (seconds) [LOG SCALE]')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb01b1f",
   "metadata": {},
   "source": [
    "Hopefully, you're convinced that:\n",
    "- We don't lose anything using method 2 imaging-wise\n",
    "- We have a benefit in using method 2, computation-wise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13b398",
   "metadata": {},
   "source": [
    "### 1.2.4 Applying the transformation to the entire timeseries at last\n",
    "\n",
    "With all this in mind, let's now apply our transformation to all our volumes! The steps are:\n",
    "\n",
    "1. Split our EPI into all individual volumes (remember: applywarp only works on a single 3D image but our EPI is 4D).\n",
    "2. Combine all transformations from EPI after motion correction all the way to standard space **once**. \n",
    "3. Use applywarp for every volume, passing the motion correction transform of this volume and the EPI > standard space warp\n",
    "4. Combine back all volumes as a single 4D EPI in standard space\n",
    "\n",
    "Let's make sure you understand why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab3867-3c50-4c0a-b02b-956d43b434fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_MCQ(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc76ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split our starting EPI volume across time \n",
    "split_target = original_epi\n",
    "split_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_split')\n",
    "\n",
    "subprocess.run(['fslsplit', split_target, split_name, '-t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a9109-c717-4036-8700-5c4078e5cf5f",
   "metadata": {},
   "source": [
    "Let's have a look at our folder structure now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da7bff-27b1-4879-bc99-69e524e10b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root,max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0b151-739c-4f43-9aa8-7c4553c173e9",
   "metadata": {},
   "source": [
    "As you can see, a lot of new volumes have appeared. These are the split, individual volumes of our EPI.\n",
    "\n",
    "Great, let's now combine the different transforms EXCEPT motion correction, with method 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f4766-1b6f-4ec5-8077-2bb98176bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Let's combine the different transforms EXCEPT motion correction!\n",
    "warp_name = op.join(preproc_root, 'sub-001', 'func', 'sub-001_epi_moco_2_std_warp')\n",
    "\n",
    "print(\"Starting to combine transforms...\")\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "print(\"Done, moving on to application of transforms...\")\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "# Now apply transformation to all our volumes.\n",
    "# We will remember the volumes as well, to group them back afterwards.\n",
    "##########\n",
    "\n",
    "# Notice that we are sorting the volumes here! This is important, to make sure we don't get them in random order :)\n",
    "split_vols = sorted(glob.glob(op.join(preproc_root, 'sub-001', 'func', '*_bold_split*')))\n",
    "\n",
    "\n",
    "# Define a function that wraps subprocess.run()\n",
    "def run_subprocess(split_vol, vol_nbr):\n",
    "    \"\"\"\n",
    "    SAFETY GOGGLES ON\n",
    "    This function launches applywarp in parallel to reach complete result quicker\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    split_vol: str\n",
    "        Path to the volume on which to apply the transformation\n",
    "    vol_nbr: str\n",
    "        Number of the volume in the timeserie. Useful to reorder volumes after the fact, since parallelisation does not honour order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vol: str\n",
    "        Path to the transformed volume\n",
    "    vol_nbr: str\n",
    "        Number of the volume in the timeserie. Useful to reorder volumes after the fact.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        split_nbr = split_vol.split('_')[-1].split('.')[0].split('split')[1]\n",
    "        epi_moco = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "        out_vol= op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std_vol' + split_nbr)\n",
    "        result = subprocess.run(['applywarp', '-i', split_vol, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs', '--premat={}'.format(epi_moco)], check=True)\n",
    "        return out_vol, vol_nbr\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"applywarp for volume '{split_vol}' failed with error: {e.stderr.decode('utf-8')}\"\n",
    "\n",
    "\n",
    "produced_vols = [None]*len(split_vols)\n",
    "# Initialize ThreadPoolExecutor and the progress bar\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use tqdm to wrap the futures\n",
    "    with tqdm(total=len(split_vols)) as progress:\n",
    "        # Launch subprocesses in parallel\n",
    "        futures = {executor.submit(run_subprocess, vol,i): vol for i,vol in enumerate(split_vols)}\n",
    "\n",
    "        # Process completed tasks\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            out_vol, vol_nbr = future.result()  # Get the result of the subprocess\n",
    "            produced_vols[vol_nbr] = out_vol\n",
    "            # Update the progress bar for each completed task\n",
    "            progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2672185-f2ba-43a0-b7f4-bd027fd0e7cb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Great, now we finally have our volumes!\n",
    "We can finally analyze them, as you will see next week!\n",
    "\n",
    "### 1.2.5 Grouping the volumes together (Optional)\n",
    "\n",
    "<b>This section is purely optional, you can readily skip it if you want.</b>\n",
    "\n",
    "Our volumes are now individually separated, but we started with a 4D volume...\n",
    "Perhaps we'd like to have them back as an individual volume, to visualize them better?\n",
    "The part below aims precisely at doing that for you.\n",
    "\n",
    "In theory we should be able to group everything back. The issue is, our fMRI is now interpolated at 1 x 1 x 1 mm3 resolution, so it won't fit in RAM on this virtual machine, where a high performance computing cluster would do this without any issue.\n",
    "\n",
    "There are two answers to this issue. The first, used by fMRIprep, is to play it smart and actually never modify the fMRI's resolution.\n",
    "\n",
    "The second is to use to group back the files a memory map, that is to say a file on disk from which we only read the parts we need (VERY useful when you do not have enough RAM). Because writing to disk is very slow, we do it in a batch approach below.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C04000; color: #112A46; border-left: solid #C04000 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>⚠️  This part is TIME INTENSIVE and it is OPTIONAL ⚠️ </b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    This part will take TIME (about an hour, in fact)\n",
    "It is probably better if you jump ahead to another part of the notebook and start reading it while this is running, or just do not run it at all and remember this in case you absolutely need it later. This hour is probably better utilized somewhere else, right ? ;)</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef73514-3357-4012-99a5-a76c779878ad",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import progressbar\n",
    "\n",
    "first_vol = nib.load(produced_vols[0])\n",
    "v_shape = first_vol.get_fdata().shape\n",
    "\n",
    "preproc_root = '/home/jovyan/Data/dataset/ds004226/derivatives/preprocessed_data'\n",
    "filename = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_std.dat')\n",
    "large_array = np.memmap(filename, dtype=np.float64, mode='w+', shape=(v_shape[0],v_shape[1],v_shape[2], len(produced_vols)))\n",
    "\n",
    "batch_size = len(produced_vols)//4\n",
    "\n",
    "A = np.zeros((v_shape[0],v_shape[1],v_shape[2], batch_size))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=len(produced_vols)) as bar:\n",
    "    for batch_i in range(4):\n",
    "        print('Starting for batch {}/4'.format(batch_i+1))\n",
    "        start_batch = batch_size * batch_i\n",
    "        end_batch = min(batch_size * (batch_i+1),len(produced_vols))\n",
    "        max_len = end_batch - start_batch + 1\n",
    "        for i in range(start_batch, end_batch):\n",
    "            vol = nib.load(produced_vols[i])\n",
    "            A[:,:,:,i-start_batch] = vol.get_fdata()\n",
    "            bar.update(i)\n",
    "        large_array[:,:,:, start_batch:end_batch] = A[:,:,:,:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76578c-c971-4690-bedc-1479862f4850",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, save to Nifti file using Nibabel\n",
    "\n",
    "# Step 1: Ensure all changes of the memmap are flushed to disk and then close it\n",
    "#large_array.flush()\n",
    "#del large_array\n",
    "print(\"Done flushing mmap\")\n",
    "large_array = np.memmap(filename, dtype=np.float64, mode='r', shape=(v_shape[0],v_shape[1],v_shape[2], len(produced_vols)))\n",
    "\n",
    "# Step 2: Modify the header to indicate that we have 4D data, and specify its TR.\n",
    "header = first_vol.header.copy()  # Copy the header of the first volume (to get right resolution, affine, Q-form etc)\n",
    "header['dim'][0] = 4  # Specifies that this is a 4D dataset\n",
    "header['dim'][1:5] = large_array.shape  # Update dimensions (x, y, z, t)\n",
    "header['pixdim'][4] = 1.5  # Set the TR in the 4th dimension. You can see the TR of the data by looking at your original EPI series with fslhd, remember ;)\n",
    "print(\"Done with header\")\n",
    "\n",
    "# Step 3: Create the Nifti1 image and save it to disk\n",
    "mni_epi = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_mni.nii.gz')\n",
    "img = nib.Nifti1Image(large_array, first_vol.affine, first_vol.header)\n",
    "print(\"Done creating the image\")\n",
    "img.to_filename(mni_epi)\n",
    "print(\"Done writing it to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625691c-812f-4352-9a64-429b3f40393f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Finally, we can move on to removing all the temporary files we used, to end up with only one clean Nifti file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21f414-8b32-46c6-b216-d602c6616e41",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.system('rm -rf {}'.format(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_split*.nii.gz')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73531a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, let's check we did a proper job. If we did a proper mapping, we should definitely observe the EPI positioned on the anatomical in MNI space. How well did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8abf15",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(ref)\n",
    "fsleyesDisplay.load(produced_vols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9079c4-8ae7-4c34-b68c-1cfdba7909d7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.3 Where are we?\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Field unwarping</td><td style='text-align:justify;'>Correction distortions induced by inhomogeneities of the b0 field through maps acquired specifically to measure this field called fieldmaps.</td><td style='text-align:justify;'>FUGUE (but also FLIRT - see below)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Slice-timing correction</td><td style='text-align:justify;'>Accounting for the difference in acquisition between the slices that make up a volume to interpolate back voxels to a fixed time reference.</td><td style='text-align:justify;'>slicetimer</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec62f69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "NOTE:\n",
    "epi_reg is still using FLIRT under the hood! To be more specific, it is using flirt setup with its search cost as BBR. If you look through FLIRT's options, you'll notice that many more options are open to you:\n",
    "<img src=\"imgs/flirt_options.png\"/>\n",
    "\n",
    "Feel free to explore the effect of different search costs :) But remember: not all costs are born equal when registering images **across modalities**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18139c8b-b600-4774-ab8f-c2741779e1fe",
   "metadata": {},
   "source": [
    "## 1.4 Slice time correction\n",
    "\n",
    "The importance of this step is still being investigated in fMRI literature. See <a href=\"https://www.frontiersin.org/articles/10.3389/fnins.2019.00821/full\">here</a> for an in-depth analysis of its impact on the pipeline. One of the take-aways from this reference is that slice-time correction together with motion correction does improve results of fMRI analysis and does not hurt.\n",
    "Doing it before or after MC is not clear, as you can see in the reference above, so we're *choosing* here to showcase it after motion correction, but only time and further investigations will tell if there's a good order :)\n",
    "\n",
    "\n",
    "#### 1.4.1 A toy example\n",
    "\n",
    "To help you understand the underlying theory of slice-time correction, we will start from a rather unreasonable case on synthetic data, which will help you better visualize how slice-time correction affects the patterns.\n",
    "\n",
    "As you'll see, this step can only be performed if you have knowledge of the way in which slices were acquired. Most of the time, fortunately, this is easy to recover. If it is not present in your data, you can ask the scanner operator to find out which sequence was used for your data acquisition.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce97298-6217-43de-836a-d3f88b543fc3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run generate_smileys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb346d-3818-448a-bb04-5579adc83cec",
   "metadata": {},
   "source": [
    "First load the file named \"ground_truth_modulation.nii\" in FSLeyes to visualize it. The data is 4D, go ahead a play a bit around to see exactly what happens during the sequence! (don't forget in case of flickering to untick the \"Synchronize movie update\" option of FSLeyes).\n",
    "\n",
    "What you see is the ground truth, ie: the real phenomenon as it plays out!\n",
    "\n",
    "We now have an acquisition sequence. It performs in slices, but not in a linear order.\n",
    "Our sequence is even weirder: at a given time, not one but 9 slices are made at the same time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb2a4d-b8be-4709-abb8-727c67adb32b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import InterpolatedUnivariateSpline as Interp\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_array_asnib(array, save_name):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a Nifti file.\n",
    "    The nifti is considered to have identity transformation matrix and to be unsigned int.\n",
    "    If you wish to use this function for float data, you should remove the .astype(np.uint8) ;)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array: np.ndarray\n",
    "        The array to save\n",
    "    save_name: str\n",
    "        Path to which the array will be saved\n",
    "    \"\"\"\n",
    "    img = nib.Nifti1Image(array.astype(np.uint8), np.eye(4))\n",
    "    nib.save(img, save_name)\n",
    "    \n",
    "def check_dims(axis_len, seq_len):\n",
    "    \"\"\"\n",
    "    Checks that axis length and seq length are equal. Otherwise, raise an Exception.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    axis_len: int\n",
    "        Length of the axis (number of elements along the axis of the array in your case)\n",
    "    seq_len: int\n",
    "        Length of the sequence\n",
    "    \"\"\"\n",
    "    if axis_len != seq_len:\n",
    "        raise Exception('The number of slices in the sequence is different from the number of slices available in the axis. Are you sure this is the right axis?')\n",
    "\n",
    "def reslice_with_timings(slice_dir, slice_sequence,input_data, original_times):\n",
    "    \"\"\"\n",
    "    Perform slice timing correction according to slice sequence timing and slice direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    slice_dir: char (can be 'x', 'y', or 'z')\n",
    "        Slice direction, the encoding direction of the sequence.\n",
    "    slice_sequence: np.ndarray\n",
    "        Matrix representing which slices are acquired in which order.\n",
    "    input_data: np.ndarray\n",
    "        The matrix that was acquired by the slice sequence and which we wish to correct\n",
    "    original_times: np.ndarray\n",
    "        The times at which the images are acquired and where we would like to reinterpolate back our slices.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    output_data: np.ndarray\n",
    "        The resliced data matrix, result of the slice-timing correction.\n",
    "    \"\"\"\n",
    "    assert(original_times.size == input_data.shape[3])\n",
    "    \n",
    "    n_acqs_per_tr = slice_seq.shape[0]\n",
    "    n_multibands = slice_seq.shape[1]\n",
    "    n_slices = slice_seq.size\n",
    "    \n",
    "    r= -1\n",
    "    if slice_dir=='x':\n",
    "        # For each slice in x, interpolate!\n",
    "        n = input_data.shape[0]\n",
    "        check_dims(n, n_slices)\n",
    "        r=0\n",
    "    elif slice_dir == 'y':\n",
    "        # For each slice in y, interpolate!\n",
    "        n = input_data.shape[1]\n",
    "        check_dims(n, n_slices)\n",
    "        r=1\n",
    "    elif slice_dir == 'z':\n",
    "        # For each slice in z, interpolate!\n",
    "        n = input_data.shape[2]\n",
    "        check_dims(n, n_slices)\n",
    "        r=2    \n",
    "    else:\n",
    "        # Undefined yo\n",
    "        raise Exception('Invalid dimension! Should be x, y or z')\n",
    "    # Reshape the input data to have r as first dimension!\n",
    "    input_data = np.swapaxes(input_data, 0, r)\n",
    "    \n",
    "    output_data = np.zeros(input_data.shape)\n",
    "    print(output_data.shape)\n",
    "\n",
    "    y_s = input_data.shape[1]\n",
    "    z_s = input_data.shape[2]\n",
    "    \n",
    "    # Now, on the first axis, we will iterate over slices :)\n",
    "    for b in range(0, n_acqs_per_tr):\n",
    "        time_slice = original_times + b*1./n_acqs_per_tr\n",
    "        # For all slices acquired together in the multiband\n",
    "        slices = slice_seq[b]\n",
    "        print('---------')\n",
    "        print(time_slice)\n",
    "        print(slices)\n",
    "        for s in range(0, n_multibands):\n",
    "            sl = slices[s]\n",
    "            for y in range(0, y_s):\n",
    "                for z in range(0, z_s):\n",
    "                    lin_interper = Interp(time_slice, input_data[sl, y, z, :], k=1)\n",
    "                    output_data[sl, y, z, :] = lin_interper(original_times)\n",
    "    # Now that this very inefficient method is done, we should remember to swap back the axis :)\n",
    "    input_data =np.swapaxes(input_data, r, 0)\n",
    "    output_data=np.swapaxes(output_data, r, 0)\n",
    "\n",
    "    output_data[output_data < 0] = 0\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d0b5d-66c4-4f7b-ab31-615e67a8d08f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_seq = np.arange(0, 99).reshape((11, - 1))\n",
    "slice_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75842c17-dd90-418f-91e9-23d07b021d04",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As you can see, these slices are acquired in a sequential order that can be called either ascending or descending depending on your convention!\n",
    "It means that the slices are obtained successively.\n",
    "\n",
    "There are different types of slicing, which depends on your sequence. Notice that in our example we acquire several slices simultaneously (for example, slices 0 up to 9 are all acquired together!). This could be some multiband acquisition, for instance, but really it is mostly to help you visualize the effect of slice timing for the exercise. In practice the slices are defined to sample the signal in the most appropriate way, so our toy sequence will likely be too crude :)\n",
    "\n",
    "In any case, we had some ground truth signal, that you can visualize in ground_truth_modulation.nii (don't forget to play the movie with the box unticked!)\n",
    "\n",
    "This signal represents the true signal that we want to acquire.\n",
    "The participant steps in an MRI, and the scanner operator uses the sequence we've defined above:\n",
    "\n",
    "```\n",
    "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "       [ 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
    "       [18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
    "       [27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    "       [36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
    "       [45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
    "       [54, 55, 56, 57, 58, 59, 60, 61, 62],\n",
    "       [63, 64, 65, 66, 67, 68, 69, 70, 71],\n",
    "       [72, 73, 74, 75, 76, 77, 78, 79, 80],\n",
    "       [81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
    "       [90, 91, 92, 93, 94, 95, 96, 97, 98]])\n",
    "```\n",
    "\n",
    "What this really means is that we acquire 9 slices at the same time and then move on to the next slices to acquire, we acquire 9 of them and so on.\n",
    "\n",
    "Your task will be two-folds:\n",
    "1. Understand which axis the sequence was applied on, ie along which direction (X, Y or Z) was slicing performed\n",
    "2. With this simple knowledge, apply a slice-timing correction algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d929f-c505-4329-bd11-fe135eac193b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This cute smiley will represent our neurological data! Every time point, the behaviour is simple: the smiley's intensity increases across time!\n",
    "Here is what it looks like in FSLeyes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fea7cd-aeec-47a2-ade4-561c0faa4c2a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('ground_truth_modulation.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea02c8d-cc35-4c9a-aa6f-af1bde00643a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('ground_truth_upsampled_modulation.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a4fd0-59ef-4fa7-a934-51d7212186eb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Using the following slice sequence, we have acquired our smiley signal.\n",
    "The resulting timeseries is represented in the acquired_modulation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ffe4c-33ee-40c0-9cc1-83c93a6aeac4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('acquired_modulation.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735339a-c692-4663-a56f-b2f4f361c0ae",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Oh no! What went wrong?\n",
    "\n",
    "Well, if you think about it, nothing. It is simply that acquiring every slice takes time. During this time, the signal is evolving, so we're a little late in our acquisition, which causes the drift you're seeing.\n",
    "\n",
    "This is what you observe in the acquired modulation file. The slice that is at the top - which is the last slice - also turns out to be the one with highest value. At timestep 0, it is in fact almost equal to 10 - the value of the ground truth at timestep 1 !\n",
    "\n",
    "### 1.4.2 Correcting the delay\n",
    "\n",
    "The heart of slice-timing correction is an interpolation in time.\n",
    "Because the timing of the slices is wrong, we account for it by interpolating back to some reference time. This is how we obtain the resliced data.\n",
    "For this, we need some informations. The first is the sequence in which slices are acquired, to know the lag of each slice. We also need the axis along which slices are acquired.\n",
    "\n",
    "Your task, in this simplified example, based **only on the abnormal smiley visualization in FSLeyes** and the knowledge of the sequence (that is: a bottom-up sequence with 9 simultaneous bands in every slice), is to determine the direction of the phase encoding, ie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ad944-0cc3-4ce4-897f-511141c95c1e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a35e79-89c5-4882-8f2a-ab5030301a66",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Based on your answer, in the following cell, fill in the phase_encode variable, with either 'x', 'y' or 'z' (with the quotation marks!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22149708-1e7a-4129-bdcc-398f3cce6a9a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phase_encode = 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d93ca-9de7-4b49-ba15-f08f3fcf21cc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will now conduct slice-timing correction: the idea is simply to interpolate back the slices in time along the slice direction. Easy right? Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9035e2-8f90-4ee7-91f6-c687eb3187f1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "smile_ts = nib.load('ground_truth_modulation.nii').get_fdata()\n",
    "smile_resampled = nib.load('acquired_modulation.nii').get_fdata()\n",
    "resliced_data = reslice_with_timings(phase_encode, slice_seq, smile_resampled, np.arange(0,9))\n",
    "save_array_asnib(resliced_data.astype(np.uint8), 'resliced_data.nii')\n",
    "\n",
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load('resliced_data.nii')\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[0]).cmap = 'hot'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41757311-9e52-4659-ab2e-67c81acb06e7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Are you convinced on this toy example we did a not-too bad job?\n",
    "\n",
    "### 1.4.3  Application to real data\n",
    "\n",
    "We have shown you the basic principle, but the application to real data requires some specific informations.\n",
    "You need the following ingredients:\n",
    "- When was each slice acquired in the sequence: **(Slice timing)**\n",
    "- Along which axis were the slices acquired: **Phase direction**\n",
    "- How much time we take to acquire all slices: **TR**\n",
    "\n",
    "Let's go back to our practical dataset to extract these informations. Can you find them, when looking through the JSON sidecar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a4c56-4d26-4213-a197-2edefc620f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_json_from_file(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01188188-bc2c-478d-b747-263a105d04e9",
   "metadata": {},
   "source": [
    "This data is actually a dictionary. We can thus extract the slice timing as an array directly from it. For example, to extract TaskName, we would use:\n",
    "```python\n",
    "data['TaskName']\n",
    "```\n",
    "\n",
    "Go ahead and extract the slice timing array, and store it in the slice_timing variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fbdbb-4565-441e-ac7e-56d21b83c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_timing = data['SliceTiming'] # Replace with the appropriate key (have a look above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713d62a-fdf3-4547-9cb8-fc51cc91c5db",
   "metadata": {},
   "source": [
    "Now, we might want to know where our slices are, ie along which axis, right? Typically it is along the z-direction, but we're better off if we check! Using FSLeyes, determine how many slices each axis has **for the functional data of interest**. You should thus open the relevant functional file in FSLeyes to answer this question.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>Using FSL command line</b></p>\n",
    "<p style='text-indent: 10px;'>To figure out the dimensions of an MRI image, a faster option - if you have FSL installed directly - is to run the command line command:\n",
    "    <blockquote>fslhd [your_volume]</blockquote>\n",
    "This will give you all informations contained within the header of the NIfti file. For example, running the command for our volume will easily allow us to access the slice informations:\n",
    "    <img src=\"imgs/fslhd_capture.png\"></p>\n",
    "</span>\n",
    "</div>\n",
    "Let's compare now with the amount of slices we have in our acquisition. We can consider simply the number of timings for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa06533-f91b-45c6-be31-e60160f984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slice_timing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659a402",
   "metadata": {},
   "source": [
    "So we have 52 slices in our slice timings, and you likely found 77 slices on the X axis, 77 axis on the Y axis and 51 slices on the Z axis. As a consequence, Z is the axis where the slices were acquired!\n",
    "Great, so we know which axis we want, we know the slice timings, but we still need to know the TR. This information is also in the JSON sidecar! Extract it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14624586",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data['RepetitionTime'] # Extract the TR from the sidecar's appropriate field\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c1429",
   "metadata": {},
   "source": [
    "To now perform the correction, we need to apply FSL's slicetimer command. For this, we need to save the timings first to their own separate file! Instead of giving the slice timings, we will provide instead the slice **order** (ie which slice was done in which order) and let FSL figure out how to best correct based on this information.\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59910eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_order = np.argsort(slice_timing) + 1\n",
    "\n",
    "# Write to a file the corresponding sorted timings :)\n",
    "timing_path = op.join(preproc_root,  'sub-001', 'func', 'sub-001_task-sitrep_run-01_slice-timings.txt')\n",
    "file = open(timing_path, mode='w')\n",
    "for t in slice_order:\n",
    "    file.write(str(t) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a80368",
   "metadata": {},
   "source": [
    "Finally we can call slicetimer from a terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d929366",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_to_realign = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "output_target = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_slice-corr')\n",
    "\n",
    "subprocess.run(['slicetimer', '-i', file_to_realign, '-o', output_target, '-r', str(tr), '-d', str(3), '--ocustom={}'.format(timing_path)])\n",
    "#cmd = 'slicetimer -i ' + file_to_realign + ' -o ' + output_target + ' -r ' + str(tr) + ' -d 3 --ocustom=' + timing_path\n",
    "#os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b381dbc-4673-4ee9-8b0a-fb4e8df51028",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(file_to_realign)\n",
    "fsleyesDisplay.load(output_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c93a3c-4d12-4a9c-ad9c-dce9af067586",
   "metadata": {},
   "source": [
    "As you would notice, visual differences due to slice-timing correction are not too obvious in general.\n",
    "\n",
    "There are unfortunately cases where it can go very wrong, due to the interpolation nature of the approach.\n",
    "Consider the four images below.\n",
    "\n",
    "<div class=\"fit\">\n",
    "<img src=\"imgs/slice_uncorr.png\" max-width=\"1400px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188288d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Had we launched it on the unscrubbed data, we would really notice the impact on the first volume. <span style=\"color:red;\">Notice that you should in general **never** do this, as you would introduce lots of noise and garbage in your data.</span> Prefer to first clean all that is weird and then perform steps that might not bring a visible improvement rather than starting with data that is so bad that you can visually see changes when running above steps.\n",
    " <div class=\"row\">\n",
    "    <img src=\"imgs/slice_uncorr.png\" style=\"width:100%\">\n",
    "      <center>First volume without slice correction</center>\n",
    "    <img src=\"imgs/slice_corr.png\" style=\"width:100%\">\n",
    "       <center>First volume with slice correction: the staircase has been more or less mitigated but the result is still imperfect...</center>\n",
    "    <center>**And because of the linear interpolation, the garbage of volume 0 was spilled to volume 1!!!**</center>\n",
    "    <img src=\"imgs/vol1_slice_uncorr.png\" style=\"width:100%\">\n",
    "      <center>Second volume without slice correction</center>\n",
    "    <img src=\"imgs/vol1_slice_corr.png\" style=\"width:100%\">\n",
    "       <center>Second volume with slice correction: the result is worse than before...</center>\n",
    "</div> \n",
    "\n",
    "\n",
    "The message here is: \n",
    "- **always** perform QC between your steps\n",
    "- no algorithm can turn trash to gold. Remove the faulty volumes or you'll likely have a garbage-in garbage-out scenario. \n",
    "\n",
    "<u>Remember the 1 - 10 -100 dollar rule! It is much easier to avoid errors than compensate for them.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de77c5-c9ff-48b7-9438-27974562068c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# MRI and fMRI preprocessing: conclusion\n",
    "\n",
    "You have reached the end the preprocessing for MRI and fMRI. \n",
    "\n",
    "As you can see, there are many fairly involved steps that need to be conducted.\n",
    "\n",
    "To normalize choices and results, software solutions exist, such as the excellent <a href=\"https://fmriprep.org/en/stable/\">fmriprep</a>, to conduct automatically all steps for you while providing you with quality checks to verify that all went well, as no algorithm can replace your expert eye to inspect potential artefacts and remove them.\n",
    "\n",
    "These tools are nonetheless precious to help different groups follow same systematic choices of preprocessing, both in parameters and order of application for each method, leading to more reproducible science in the long run. (But they require a big RAM and take hours to run, which is why we've avoided them for the purpose of this tutorial :) )\n",
    "\n",
    "Let's wrap up what you have learnt.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM in one week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Fieldmap preparation</td><td style='text-align:justify;'>Field maps can be used to create a distortion field to correct...Distortions. </td><td style='text-align:justify;'>topup, FUGUE</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Coregistration</td><td style='text-align:justify;'>Realignment of fMRI volumes to anatomical space - the subject's own MRI is typically used. We can include susceptibility-distortion correction with fieldmaps. We compute this transformation only for the volume we used as reference in MCFLIRT. Then, we apply it to all other volumes in the EPI.</td><td style='text-align:justify;'>epi_reg (FLIRT with boundary-based registration)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Normalization</td><td style='text-align:justify;'>Putting the EPI in a template space, such as MNI. This is done by applying the transformation of anatomical space to MNI space, which was computed in the anatomical preprocessing. Note that we typically like to combine this transform with coregistration to do everything in one go.</td><td style='text-align:justify;'>applywarp to apply combined warps, otherwise (if going transform by transform), FLIRT with applyxfm option</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "If you have any more questions, both on these tools and on preprocessing, do not hesitate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114371c-8565-4b97-a88e-1a6d36daec47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style=\"color:red;font-size:20px;\">To release all fsleyes resources and to have interactivity below, you will now need to STOP the kernel. Start running the cells from Part 2 onwards after that.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fdce3-a223-479f-b6ce-d14af6a17285",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 2: Get acquainted with fNIRS data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a5f92-f5c6-419f-8e53-5ce8922d4a22",
   "metadata": {},
   "source": [
    "In contrast to functional magnetic resonance imaging (fMRI), functional Near-Infrared Spectroscopy (fNIRS) distinguishes itself with portability and adaptability, making it especially suitable for research involving intricate populations like infants, tasks characterized by motion, and real-world settings.\n",
    "\n",
    "Nevertheless, it is important to acknowledge that collecting fNIRS data requires rigorous preprocessing. This requirement arises from fNIRS's susceptibility to 1) superficial physiological interferences, such as those stemming from scalp blood flow and 2) motion artifacts, especially those resulting from sensor displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfee3ae-6594-4c31-9c6d-a91e65d8ab4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from mne.preprocessing.nirs import temporal_derivative_distribution_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ddcbc-ad58-46b9-99e1-955885332020",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.1 Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3c43f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.1.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3790ca-1acb-48e1-bb57-61c2d4200991",
   "metadata": {},
   "source": [
    "We will work on a set of hemodynamic data measured on one subject during a finger tapping paradigm with three conditions: 1) Tapping the left thumb to fingers, 2) Tapping the right thumb to fingers and 3) A control when nothing happens. Each tapping lasts 5 seconds and there are 30 trials in each condition.The recording was performed using fNIRS sensors located over motor areas of the cortex. \n",
    "\n",
    "Data were provided by Luke, R., & McAlpine, D. (2021). fNIRS Finger Tapping Data in BIDS Format (Version v0.0.1) (https://doi.org/10.5281/zenodo.5529797),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5d71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.1.2 Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eca6c2-6752-42d0-8a72-c641a7172cd4",
   "metadata": {},
   "source": [
    "Load the dataset by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7319ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/jovyan/Data/mne_data/MNE-fNIRS-motor-data/Participant-1\n",
      "Reading 0 ... 23238  =      0.000 ...  2974.464 secs...\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import os.path as op\n",
    "#Download dataset\n",
    "fnirs_data_folder = mne.datasets.fnirs_motor.data_path(path='/home/jovyan/Data/mne_data/')\n",
    "#Get the path for the dataset folder \n",
    "fnirs_data_folder=op.join(fnirs_data_folder, 'Participant-1')\n",
    "#Load the dataset \n",
    "raw_intensity = mne.io.read_raw_nirx(fnirs_data_folder, verbose=True, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b267d19",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.1.3 Recording setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c16446-b605-4aef-b3d6-c4481d1d2b34",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Display and read information on the recording setting such as the number of channels, the file duration and the sampling frequency by runing the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa256c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "    const toggleVisibility = (className) => {\n",
       "\n",
       "  const elements = document.querySelectorAll(`.${className}`)\n",
       "\n",
       "  elements.forEach(element => {\n",
       "    if (element.classList.contains('repr-section-header')) {\n",
       "      // Don't collapse the section header row.\n",
       "       return\n",
       "    }\n",
       "    if (element.classList.contains('repr-element-collapsed')) {\n",
       "      // Force a reflow to ensure the display change takes effect before removing the class\n",
       "      element.classList.remove('repr-element-collapsed')\n",
       "      element.offsetHeight // This forces the browser to recalculate layout\n",
       "      element.classList.remove('repr-element-faded')\n",
       "    } else {\n",
       "      // Start transition to hide the element\n",
       "      element.classList.add('repr-element-faded')\n",
       "      element.addEventListener('transitionend', handler = (e) => {\n",
       "        if (e.propertyName === 'opacity' && getComputedStyle(element).opacity === '0.2') {\n",
       "          element.classList.add('repr-element-collapsed')\n",
       "          element.removeEventListener('transitionend', handler)\n",
       "        }\n",
       "      });\n",
       "    }\n",
       "  });\n",
       "\n",
       "  // Take care of button (adjust caret)\n",
       "  const button = document.querySelectorAll(`.repr-section-header.${className} > th.repr-section-toggle-col > button`)[0]\n",
       "  button.classList.toggle('collapsed')\n",
       "\n",
       "  // Take care of the tooltip of the section header row\n",
       "  const sectionHeaderRow = document.querySelectorAll(`tr.repr-section-header.${className}`)[0]\n",
       "  sectionHeaderRow.classList.toggle('collapsed')\n",
       "  sectionHeaderRow.title = sectionHeaderRow.title === 'Hide section' ? 'Show section' : 'Hide section'\n",
       "}\n",
       "</script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "    table.repr.table.table-hover.table-striped.table-sm.table-responsive.small {\n",
       "  /* Don't make rows wider than they need to be. */\n",
       "  display: inline;\n",
       "}\n",
       "\n",
       "table > tbody > tr.repr-element > td {\n",
       "  /* Apply a tighter layout to the table cells. */\n",
       "  padding-top: 0.1rem;\n",
       "  padding-bottom: 0.1rem;\n",
       "  padding-right: 1rem;\n",
       "}\n",
       "\n",
       "table > tbody > tr > td.repr-section-toggle-col {\n",
       "  /* Remove background and border of the first cell in every row\n",
       "     (this row is only used for the collapse / uncollapse caret)\n",
       "\n",
       "     TODO: Need to find a good solution for VS Code that works in both\n",
       "           light and dark mode. */\n",
       "  border-color: transparent;\n",
       "  --bs-table-accent-bg: transparent;\n",
       "}\n",
       "\n",
       "tr.repr-section-header {\n",
       "  /* Remove stripes from section header rows */\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "  --bs-table-striped-bg: transparent;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       "tr.repr-section-header > th {\n",
       "  text-align: left !important;\n",
       "  vertical-align: middle;\n",
       "}\n",
       "\n",
       ".repr-element, tr.repr-element > td {\n",
       "  opacity: 1;\n",
       "  text-align: left !important;\n",
       "}\n",
       "\n",
       ".repr-element-faded {\n",
       "  transition: 0.3s ease;\n",
       "  opacity: 0.2;\n",
       "}\n",
       "\n",
       ".repr-element-collapsed {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "/* Collapse / uncollapse button and the caret it contains. */\n",
       ".repr-section-toggle-col button {\n",
       "  cursor: pointer;\n",
       "  width: 1rem;\n",
       "  background-color: transparent;\n",
       "  border-color: transparent;\n",
       "}\n",
       "\n",
       "span.collapse-uncollapse-caret {\n",
       "  width: 1rem;\n",
       "  height: 1rem;\n",
       "  display: block;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: left;\n",
       "  background-size: contain;\n",
       "}\n",
       "\n",
       "/* The collapse / uncollapse carets were copied from the free Font Awesome collection and adjusted. */\n",
       "\n",
       "/* Default to black carets for light mode */\n",
       ".repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "}\n",
       "\n",
       ".repr-section-toggle-col\n",
       "  > button:not(.collapsed)\n",
       "  > span.collapse-uncollapse-caret {\n",
       "  background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"black\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "}\n",
       "\n",
       "/* Use white carets for dark mode */\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .repr-section-toggle-col > button.collapsed > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M246.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-9.2-9.2-22.9-11.9-34.9-6.9s-19.8 16.6-19.8 29.6l0 256c0 12.9 7.8 24.6 19.8 29.6s25.7 2.2 34.9-6.9l128-128z\"/></svg>');\n",
       "  }\n",
       "\n",
       "  .repr-section-toggle-col\n",
       "    > button:not(.collapsed)\n",
       "    > span.collapse-uncollapse-caret {\n",
       "    background-image: url('data:image/svg+xml;charset=utf8,<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path fill=\"white\" d=\"M137.4 374.6c12.5 12.5 32.8 12.5 45.3 0l128-128c9.2-9.2 11.9-22.9 6.9-34.9s-16.6-19.8-29.6-19.8L32 192c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9l128 128z\"/></svg>');\n",
       "  }\n",
       "}\n",
       "\n",
       ".channel-names-btn {\n",
       "  padding: 0;\n",
       "  border: none;\n",
       "  background: none;\n",
       "  text-decoration: underline;\n",
       "  text-decoration-style: dashed;\n",
       "  cursor: pointer;\n",
       "  color: #0d6efd;\n",
       "}\n",
       "\n",
       ".channel-names-btn:hover {\n",
       "  color: #0a58ca;\n",
       "}\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<table class=\"repr table table-hover table-striped table-sm table-responsive small\">\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header general-e2954d1b-8853-471d-aea9-8794d18bc326\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('general-e2954d1b-8853-471d-aea9-8794d18bc326')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>General</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-e2954d1b-8853-471d-aea9-8794d18bc326 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Filename(s)</td>\n",
       "    <td>\n",
       "        \n",
       "        Participant-1\n",
       "        \n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-e2954d1b-8853-471d-aea9-8794d18bc326 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>MNE object type</td>\n",
       "    <td>RawNIRX</td>\n",
       "</tr>\n",
       "<tr class=\"repr-element general-e2954d1b-8853-471d-aea9-8794d18bc326 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Measurement date</td>\n",
       "    \n",
       "    <td>2019-11-02 at 13:16:16 UTC</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-e2954d1b-8853-471d-aea9-8794d18bc326 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Participant</td>\n",
       "    \n",
       "    \n",
       "    <td>P1</td>\n",
       "    \n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-e2954d1b-8853-471d-aea9-8794d18bc326 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Experimenter</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header acquisition-956b0ecb-33d1-4a97-b114-19ae7cad5401\" \n",
       "    title=\"Hide section\"  onclick=\"toggleVisibility('acquisition-956b0ecb-33d1-4a97-b114-19ae7cad5401')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Acquisition</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element acquisition-956b0ecb-33d1-4a97-b114-19ae7cad5401 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Duration</td>\n",
       "    <td>00:49:35 (HH:MM:SS)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-956b0ecb-33d1-4a97-b114-19ae7cad5401 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Sampling frequency</td>\n",
       "    <td>7.81 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-956b0ecb-33d1-4a97-b114-19ae7cad5401 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Time points</td>\n",
       "    <td>23,239</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header channels-2234d309-42dd-4042-af02-32ffe21df775\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('channels-2234d309-42dd-4042-af02-32ffe21df775')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Channels</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-2234d309-42dd-4042-af02-32ffe21df775 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>fNIRS (CW amplitude)</td>\n",
       "    <td>\n",
       "        <button class=\"channel-names-btn\" onclick=\"alert('Good fNIRS (CW amplitude):\\n\\nS1_D1&nbsp;760, S1_D1&nbsp;850, S1_D2&nbsp;760, S1_D2&nbsp;850, S1_D3&nbsp;760, S1_D3&nbsp;850, S1_D9&nbsp;760, S1_D9&nbsp;850, S2_D1&nbsp;760, S2_D1&nbsp;850, S2_D3&nbsp;760, S2_D3&nbsp;850, S2_D4&nbsp;760, S2_D4&nbsp;850, S2_D10&nbsp;760, S2_D10&nbsp;850, S3_D2&nbsp;760, S3_D2&nbsp;850, S3_D3&nbsp;760, S3_D3&nbsp;850, S3_D11&nbsp;760, S3_D11&nbsp;850, S4_D3&nbsp;760, S4_D3&nbsp;850, S4_D4&nbsp;760, S4_D4&nbsp;850, S4_D12&nbsp;760, S4_D12&nbsp;850, S5_D5&nbsp;760, S5_D5&nbsp;850, S5_D6&nbsp;760, S5_D6&nbsp;850, S5_D7&nbsp;760, S5_D7&nbsp;850, S5_D13&nbsp;760, S5_D13&nbsp;850, S6_D5&nbsp;760, S6_D5&nbsp;850, S6_D7&nbsp;760, S6_D7&nbsp;850, S6_D8&nbsp;760, S6_D8&nbsp;850, S6_D14&nbsp;760, S6_D14&nbsp;850, S7_D6&nbsp;760, S7_D6&nbsp;850, S7_D7&nbsp;760, S7_D7&nbsp;850, S7_D15&nbsp;760, S7_D15&nbsp;850, S8_D7&nbsp;760, S8_D7&nbsp;850, S8_D8&nbsp;760, S8_D8&nbsp;850, S8_D16&nbsp;760, S8_D16&nbsp;850')\" title=\"(Click to open in popup)&#13;&#13;S1_D1&nbsp;760, S1_D1&nbsp;850, S1_D2&nbsp;760, S1_D2&nbsp;850, S1_D3&nbsp;760, S1_D3&nbsp;850, S1_D9&nbsp;760, S1_D9&nbsp;850, S2_D1&nbsp;760, S2_D1&nbsp;850, S2_D3&nbsp;760, S2_D3&nbsp;850, S2_D4&nbsp;760, S2_D4&nbsp;850, S2_D10&nbsp;760, S2_D10&nbsp;850, S3_D2&nbsp;760, S3_D2&nbsp;850, S3_D3&nbsp;760, S3_D3&nbsp;850, S3_D11&nbsp;760, S3_D11&nbsp;850, S4_D3&nbsp;760, S4_D3&nbsp;850, S4_D4&nbsp;760, S4_D4&nbsp;850, S4_D12&nbsp;760, S4_D12&nbsp;850, S5_D5&nbsp;760, S5_D5&nbsp;850, S5_D6&nbsp;760, S5_D6&nbsp;850, S5_D7&nbsp;760, S5_D7&nbsp;850, S5_D13&nbsp;760, S5_D13&nbsp;850, S6_D5&nbsp;760, S6_D5&nbsp;850, S6_D7&nbsp;760, S6_D7&nbsp;850, S6_D8&nbsp;760, S6_D8&nbsp;850, S6_D14&nbsp;760, S6_D14&nbsp;850, S7_D6&nbsp;760, S7_D6&nbsp;850, S7_D7&nbsp;760, S7_D7&nbsp;850, S7_D15&nbsp;760, S7_D15&nbsp;850, S8_D7&nbsp;760, S8_D7&nbsp;850, S8_D8&nbsp;760, S8_D8&nbsp;850, S8_D16&nbsp;760, S8_D16&nbsp;850\">\n",
       "            56\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-2234d309-42dd-4042-af02-32ffe21df775 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Head & sensor digitization</td>\n",
       "    \n",
       "    <td>31 points</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-section-header filters-c369ba5c-10b2-48be-82b9-f29246b338c4\"  title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('filters-c369ba5c-10b2-48be-82b9-f29246b338c4')\">\n",
       "    <th class=\"repr-section-toggle-col\">\n",
       "        <button>\n",
       "            \n",
       "            <span class=\"collapse-uncollapse-caret\"></span>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Filters</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element filters-c369ba5c-10b2-48be-82b9-f29246b338c4 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Highpass</td>\n",
       "    <td>0.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-c369ba5c-10b2-48be-82b9-f29246b338c4 \">\n",
       "    <td class=\"repr-section-toggle-col\"></td>\n",
       "    <td>Lowpass</td>\n",
       "    <td>3.91 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>"
      ],
      "text/plain": [
       "<RawNIRX | Participant-1, 56 x 23239 (2974.5 s), ~10.0 MB, data loaded>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display recording setting\n",
    "raw_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdfaeb0-5c44-4216-bd66-333ad86f90dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactive_MCQ(4,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d3ad9-382b-49ab-9d4a-4a9becbc7bb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The placement of fNIRS sensors holds significance for achieving **a good spatial resolution** and an **accurate sensor placement**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374edb6c",
   "metadata": {},
   "source": [
    "Let's take a look at the locations of sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c2c76c-0689-4224-940f-36790c5e2e7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvista[jupyter] in /opt/conda/lib/python3.11/site-packages (0.44.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (10.0.0)\n",
      "Requirement already satisfied: pooch in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (1.8.2)\n",
      "Requirement already satisfied: scooby>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (0.10.0)\n",
      "Requirement already satisfied: vtk in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (9.3.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (4.8.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (7.8.4)\n",
      "Requirement already satisfied: jupyter-server-proxy in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (4.1.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (1.5.8)\n",
      "Requirement already satisfied: trame>=2.5.2 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (3.6.5)\n",
      "Requirement already satisfied: trame-client>=2.12.7 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (3.3.2)\n",
      "Requirement already satisfied: trame-server>=2.11.7 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (3.2.3)\n",
      "Requirement already satisfied: trame-vtk>=2.5.8 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (2.8.10)\n",
      "Requirement already satisfied: trame-vuetify>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from pyvista[jupyter]) (2.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista[jupyter]) (2.8.2)\n",
      "Requirement already satisfied: wslink>=2.1.3 in /opt/conda/lib/python3.11/site-packages (from trame>=2.5.2->pyvista[jupyter]) (2.2.1)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.11/site-packages (from trame-server>=2.11.7->pyvista[jupyter]) (10.2.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->pyvista[jupyter]) (0.1.4)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->pyvista[jupyter]) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->pyvista[jupyter]) (5.11.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.9 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->pyvista[jupyter]) (3.6.9)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->pyvista[jupyter]) (8.16.1)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->pyvista[jupyter]) (1.1.10)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from jupyter-server-proxy->pyvista[jupyter]) (3.9.5)\n",
      "Requirement already satisfied: jupyter-server>=1.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server-proxy->pyvista[jupyter]) (2.8.0)\n",
      "Requirement already satisfied: simpervisor>=1.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server-proxy->pyvista[jupyter]) (1.0.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.11/site-packages (from jupyter-server-proxy->pyvista[jupyter]) (6.3.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from pooch->pyvista[jupyter]) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from pooch->pyvista[jupyter]) (2.31.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (4.8.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (4.0.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (23.1.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (3.1.2)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (8.4.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (5.4.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (7.9.2)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (5.9.2)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.17.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (25.1.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.11/site-packages (from jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.6.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.1->pyvista[jupyter]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista[jupyter]) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista[jupyter]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista[jupyter]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista[jupyter]) (2024.8.30)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.11/site-packages (from widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (7.0.6)\n",
      "Requirement already satisfied: msgpack<2,>=1 in /opt/conda/lib/python3.11/site-packages (from wslink>=2.1.3->trame>=2.5.2->pyvista[jupyter]) (1.0.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->jupyter-server-proxy->pyvista[jupyter]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->jupyter-server-proxy->pyvista[jupyter]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->jupyter-server-proxy->pyvista[jupyter]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->jupyter-server-proxy->pyvista[jupyter]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->jupyter-server-proxy->pyvista[jupyter]) (1.9.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.8.3)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (4.19.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (6.0.1)\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.30.2)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (3.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.18.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (2.25.0)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (4.0.7)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (0.2.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.2.8)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->pyvista[jupyter]) (0.2.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.5.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (0.10.6)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.4)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.13)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (6.25.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (2.13.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (0.9.14)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.21)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (1.8.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.9->ipywidgets->pyvista[jupyter]) (5.9.5)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.0->jupyter-server-proxy->pyvista[jupyter]) (2.8.19.14)\n",
      "Requirement already satisfied: pyvistaqt in /opt/conda/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: pyvista>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from pyvistaqt) (0.44.1)\n",
      "Requirement already satisfied: QtPy>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from pyvistaqt) (2.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (10.0.0)\n",
      "Requirement already satisfied: pooch in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (1.8.2)\n",
      "Requirement already satisfied: scooby>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (0.10.0)\n",
      "Requirement already satisfied: vtk in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (9.3.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from pyvista>=0.32.0->pyvistaqt) (4.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from QtPy>=1.9.0->pyvistaqt) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from pooch->pyvista>=0.32.0->pyvistaqt) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from pooch->pyvista>=0.32.0->pyvistaqt) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.1->pyvista>=0.32.0->pyvistaqt) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista>=0.32.0->pyvistaqt) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista>=0.32.0->pyvistaqt) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista>=0.32.0->pyvistaqt) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->pooch->pyvista>=0.32.0->pyvistaqt) (2024.8.30)\n",
      "Requirement already satisfied: pyqt5 in /opt/conda/lib/python3.11/site-packages (5.15.11)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.15 in /opt/conda/lib/python3.11/site-packages (from pyqt5) (12.15.0)\n",
      "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /opt/conda/lib/python3.11/site-packages (from pyqt5) (5.15.15)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.11/site-packages (5.6.0)\n",
      "Requirement already satisfied: pyqt5 in /opt/conda/lib/python3.11/site-packages (5.15.11)\n",
      "Requirement already satisfied: traitlets!=5.2.1,!=5.2.2 in /opt/conda/lib/python3.11/site-packages (from qtconsole) (5.11.2)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.11/site-packages (from qtconsole) (5.4.0)\n",
      "Requirement already satisfied: jupyter-client>=4.1 in /opt/conda/lib/python3.11/site-packages (from qtconsole) (8.4.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.11/site-packages (from qtconsole) (2.16.1)\n",
      "Requirement already satisfied: ipykernel>=4.1 in /opt/conda/lib/python3.11/site-packages (from qtconsole) (6.25.2)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from qtconsole) (2.4.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from qtconsole) (23.2)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.15 in /opt/conda/lib/python3.11/site-packages (from pyqt5) (12.15.0)\n",
      "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /opt/conda/lib/python3.11/site-packages (from pyqt5) (5.15.15)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (1.8.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (8.16.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (1.5.8)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.1->qtconsole) (6.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from jupyter-client>=4.1->qtconsole) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.11/site-packages (from jupyter-core->qtconsole) (3.11.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.19.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (3.0.39)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel>=4.1->qtconsole) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=4.1->qtconsole) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=4.1->qtconsole) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=4.1->qtconsole) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=4.1->qtconsole) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pyvista[jupyter]\"\n",
    "!pip install pyvistaqt\n",
    "!pip install pyqt5\n",
    "!pip install qtconsole pyqt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8916c88b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-jovyan'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pyvistaqt 3d backend.\n",
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "Channel types::\tfnirs_cw_amplitude: 56\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "subjects_dir = op.join(mne.datasets.sample.data_path('/home/jovyan/Data/mne_data'), \"subjects\")\n",
    "\n",
    "brain = mne.viz.Brain(\n",
    "subject=\"fsaverage\", subjects_dir=subjects_dir, background=\"black\", cortex=\"0.5\"\n",
    ")\n",
    "brain.add_sensors(\n",
    "    raw_intensity.info,\n",
    "    trans=\"fsaverage\",\n",
    "    fnirs=[\"channels\", \"pairs\", \"sources\", \"detectors\"],\n",
    ")\n",
    "brain.show_view(azimuth=20, elevation=60, distance=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3570f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style=\"color:green\"> See below the expected 3D visualization of the fNIRS sensors placed over the head: </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c88544-584c-4965-8d65-e9e9a34112d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"imgs/3Dmap.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08096f8",
   "metadata": {},
   "source": [
    "In the 3D visualization, we represent source-detector pairs or channels as lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23aec-dbc9-458b-a5de-1bd8f32cb952",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Multiple Choice Question</span>\n",
    "\n",
    "*What is the cortical region covered by this measurement ?*\n",
    "1. Motor cortex\n",
    "2. Visual cortex\n",
    "\n",
    "   <p style=\"color:green\"> In this finger tapping task, sensors are placed over the motor cortex. This region is located in the dorsal portion of the frontal lobe. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc93e0b",
   "metadata": {},
   "source": [
    "### 2.1.4 Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c02f5-2d5a-44e7-9c09-9b17f0690c1b",
   "metadata": {},
   "source": [
    "Let's now have a look at the experimental design used.\n",
    "\n",
    "The experimental designs most commonly employed by auditory fNIRS researchers are block- and event-related designs. In an event related design, each task is presented individually for a short amount of time e.g., 3 seconds. In this way, tasks can be more  randomized,  rather  than  being  blocked  together  by  condition. Conversely, in a block-related design, blocks of tasks, each lasting at least 10 seconds, recur before intervals of rest. \n",
    "\n",
    "The choice of the experimental design depends on a range of factors, including the statistical power, the duration of the experiment, and whether the design provides the flexibility to study the effect of interest. While the block design might lead to higher detection power, it can also induce learning and boredom effects which may bias the results. On the other hand, event-related designs reduce the effects of learning, boredom while exhibiting loss in detection power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd3413",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Open question</span>\n",
    "\n",
    "*What are the analytical challenges associated with employing a block-related experimental design featuring continuous stimulation and brief rest blocks?*\n",
    "\n",
    "   <p style=\"color:green\"> In a block-related experimental design with continuous stimulation and brief rest blocks, we can end-up having a non-linear summation of the hemodynamic responses to stimuli. This non-linearity can cause a mismatch between the predicted  summation of responses obtained with linear approaches (e.g., averaging or general linear modelling) and the actual observed data. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6d506-59bc-4300-b1ec-e0b55747c4f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Run the next cell to display the sequence of events used in this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9690b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the event annotations\n",
    "events, event_dict=mne.events_from_annotations(raw_intensity,verbose=False)\n",
    "#Assign each label to the event (based on recording setting)\n",
    "event_dict={'Control':1,'Tapping/Left':4,'Tapping/Right':3,'ExperimentEnds':2}\n",
    "#Display the sequence of events \n",
    "plt.rcParams[\"figure.figsize\"]=(10,6)\n",
    "mne.viz.plot_events(events,event_id=event_dict,sfreq=raw_intensity.info['sfreq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213d4ad-e49f-4cf3-b4ab-c4ef1cb6ef0a",
   "metadata": {},
   "source": [
    "As you can see, there is a significant gap or interstimulus interval between each individual stimulus. We can thus conclude that an event-related design was employed in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2a3f7-feec-4b81-8128-9ed70614ab17",
   "metadata": {},
   "source": [
    "Now that we have gained an understanding of the experiment and recording processes, we can start the visualization of the raw data collected. To proceed, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure window\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "#Plot time-series of raw light intensity data \n",
    "mne.viz.plot_raw(raw_intensity,start=120,duration=80,n_channels=56,show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687ea7c",
   "metadata": {},
   "source": [
    "In this plot, we can observe the time-series of the data collected, represented as the change in light intensity resulting from light absorption at specific wavelengths. Oxygenated hemoglobin (HbO) absorbs light at \"850\"nm, while deoxygenated hemoglobin (HbR) absorbs light at \"760\"nm. Each channel comprises a pair consisting of a source (labeled as \"S\" in the plot) and a detector (labeled as \"D\" in the plot). \n",
    "\n",
    "Here, we are showing all the channels (y-axis) but, for clarity, we are only focusing on a specific timeframe, spanning from 120 to 200 seconds (x-axis). If you wish to customize the visualization parameters, please refer to the documentation of [mne.viz.plot_raw](https://mne.tools/stable/generated/mne.viz.plot_raw.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33907f00",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Open question</span>\n",
    "\n",
    "*Visually, are you able to localize a motion artifact?*\n",
    "\n",
    "   <p style=\"color:green\"> There are two types of motion artefacts we can visualize in this graph: a spike between 185-190s and a baseline shift in the S8_D8 850 channel. These two types of artefact can easily happen in motor tasks. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23001d",
   "metadata": {},
   "source": [
    "## 2.2 Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcd271",
   "metadata": {},
   "source": [
    "A typical preprocessing pipeline for fNIRS data includes the following key steps:\n",
    "\n",
    "<hr>\n",
    "\n",
    "*Step 1: Raw Intensity to Optical Density Conversion*\n",
    "\n",
    "<hr> \n",
    "\n",
    "*Step 2: Motion Artifact Removal*\n",
    "\n",
    "<p style='margin-top:1em;'>💡 This step is more effective when performed at the beginning of the pipeline to prevent the propagation of errors across wavelengths and minimizes cross talk between signals obtained at different wavelengths.\n",
    "\n",
    "<hr> \n",
    "\n",
    "*Step 3: Optical Density to Concentration Conversion*\n",
    "\n",
    "<hr>\n",
    "\n",
    "*Step 4: Physiological Oscillation Filtering*\n",
    "\n",
    "<p style='margin-top:1em;'>💡 This step is more effective when performed after the conversion step using the modified Beer-Lambert law, as physiological sources of error impact HbO and HbR in a manner consistent with the Beer-Lambert equation.\n",
    "    \n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254195bd",
   "metadata": {},
   "source": [
    "### 2.2.1 Converting raw intensity data to optical density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b268b",
   "metadata": {},
   "source": [
    "The first step is to convert the changea in light intensity into changes in optical density using the modified Beer-Lambert law (Eq 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547df8e-0e72-45ec-8e38-4ac81da9f2fd",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "OD_\\lambda - OD_{R\\lambda} = log\\frac{I_o}{I} (1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e0262-9fb3-4f55-9644-d980fa138240",
   "metadata": {},
   "source": [
    "With:\n",
    "- $OD_\\lambda$ the optical density of the medium for a given wavelength $\\lambda$\n",
    "- $OD_{R\\lambda}$ the optical density of light scattering within human tissue\n",
    "- $I_o$ the incident light intensity \n",
    "- $I$ the transmitted light intensity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7931b-3449-4789-8ca1-49f54afa377e",
   "metadata": {},
   "source": [
    "Run the next cell to convert raw intensity into optical density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e624f-b7bb-4f19-8da6-6480a56cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert raw intensity into optical density data\n",
    "raw_od=mne.preprocessing.nirs.optical_density(raw_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541499a",
   "metadata": {},
   "source": [
    "### 2.2.2 Correcting motion artefacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921d012",
   "metadata": {},
   "source": [
    "In fNIRS data, two types of artifacts are commonly encountered: baseline shifts (indicating sensor displacement without returning to the initial position) and spike artifacts (characterized by sensor oscillations). As you correctly guessed earlier, the abrupt shift occurring around ~190 seconds in the time-series plot above corresponds to a spike artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5a755",
   "metadata": {},
   "source": [
    "Here, we will employ a technique known as Temporal Derivative Distribution Repair (TDDR), which is designed to address motion artefacts. TDDR uses the temporal derivative of the signal to correct the signal. \n",
    "\n",
    "For a more comprehensive understanding of the TDDR method and its application, you can refer to the following paper:\n",
    "- Frank A. Fishburn, Ruth S. Ludlum, Chandan J. Vaidya, and Andrei V. Medvedev. \"Temporal Derivative Distribution Repair (TDDR): A Motion Correction Method for fNIRS.\" NeuroImage, 184:171–179, 2019. doi:10.1016/j.neuroimage.2018.09.025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e26b3a",
   "metadata": {},
   "source": [
    "Run the cell below to apply the TDDR on the optical density data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply TDDR on optical density data\n",
    "raw_od_preproc=temporal_derivative_distribution_repair(raw_od)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346e30e",
   "metadata": {},
   "source": [
    "Execute the following cell to visualize the optical density data after applying TDDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure window\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "#Plot time-series of optical density data following TDDR application\n",
    "mne.viz.plot_raw(raw_od_preproc,show_scrollbars=False,start=120,n_channels=56,duration=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3546a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; text-decoration:underline;\">Open question</span>\n",
    "\n",
    "*From a visual standpoint, can we conclude that TDDR successfully eliminated the motion artifacts from your data?*\n",
    "\n",
    " <p style=\"color:green\"> While the TDDR corrected the baseline shift seen in some of the channels (e.g., S8_D8 850) it did not remove the spikes from the signal. This low sensitivity to motion-related spikes is actually one of the limits of the TDDR method. More robust methods that are however not proposed in mne-nirs library are the wavelet filtering or the spline interpolation.                                                                                                               For a comparison of motion artefact correction techniques, you can refer to the following paper:\n",
    "    Cooper, R. J., Selb, J., Gagnon, L., Phillip, D., Schytz, H. W., Iversen, H. K., ... & Boas, D. A. (2012). A systematic comparison of motion artifact correction techniques for functional near-infrared spectroscopy. Frontiers in neuroscience, 6, 147.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fad549",
   "metadata": {},
   "source": [
    "### 2.2.3 Converting optical density to concentration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3a0f4",
   "metadata": {},
   "source": [
    "The changes in optical density are then converted into changes of concentration using the modified beer lambert law (Eq 1): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ed4b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta c = \\frac{\\Delta OD_\\lambda }{\\epsilon_\\lambda lB} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91a1f2",
   "metadata": {},
   "source": [
    "Whith:\n",
    "- $\\Delta$ c the molar concentration change (in M)\n",
    "- $\\epsilon_\\lambda$ the moral absorption coefficient (in $M^-1 cm^-1$) for a given wavelength $\\lambda$\n",
    "- l the the optical pathlength \n",
    "- B the pathlength correction factor\n",
    "\n",
    "For more information on the equation, see the reference: Delpy DT, Cope M, Zee P van der, Arridge S, Wray S, Wyatt J. Estimation of optical pathlength through tissue from direct time of flight measurements. Phys Med Biol 1988; 33: 1433-1442."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f1a01",
   "metadata": {},
   "source": [
    "Run the cell below to compute the concentration changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957ebae-707c-46b7-b342-41d81f33356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required beer-lambert law related function\n",
    "from mne.preprocessing.nirs import beer_lambert_law\n",
    "#Convert optical density into concentration change (hemodnyamic data)\n",
    "raw_haemo=beer_lambert_law(raw_od_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fc5db",
   "metadata": {},
   "source": [
    "### 2.2.4 Filtering out physiological oscillations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba952ae1-d0dc-4962-89bf-8a3aa842eea4",
   "metadata": {},
   "source": [
    "Let's now filter out physiological oscillations.\n",
    "\n",
    "To begin, we'll assess the extent of physiological oscillations within the data using power spectral density (PSD) analysis. Based on the sequence of events outlined previously, we note that stimuli for a same condition (e.g., tapping right) are presented at a rate of approximately once every 1/100 seconds. Consequently, we should anticipate a hemodynamic response to this specific stimulus occurring at this particular frequency. Any additional components identified in the PSD analysis can be attributed to other forms of oscillations, including physiological ones.\n",
    "\n",
    "Run the following cell to initiate the signal decomposition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the PSD of non-filtered hemodynamic data \n",
    "fig = spectrum = raw_haemo.compute_psd().plot(average = True)\n",
    "fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54679-bb1a-4005-bb96-248cd96b2b8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The PSD shows a very low peak which may correspond to the hemodynamic responses to task stimulations. On the other hand, the presence of a frequency peak at 1.25 Hz aligns with the heart rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ac0a6-3c4b-4e03-b7ac-389dc8a01cb8",
   "metadata": {},
   "source": [
    "Based on the information provided, apply a low-pass filter to retain only the hemodynamic responses. You may need to utilize the filter method available in the mne.io.Raw class as outlined in the MNE documentation (https://mne.tools/stable/generated/mne.io.Raw.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364095b0",
   "metadata": {},
   "source": [
    "<p style=\"color:green\"> MNE library provides two methods for filtering raw data: filter_data() function or the method filter. Here, we will utilize the filter() method, as it is more user-friendly and integrates seamlessly with subsequent analysis steps. \n",
    "\n",
    "<p style=\"color:green\"> In the context of event-related designs, the goal is to extract hemodynamic responses to specific events. In our case, we aim to compare the hemodynamic responses to different events. \n",
    "Therefore, we have three distinct signals of interest: hemodynamic responses to 1) tapping left, 2) tapping right, and 3) control events. Maintaining the stimulation frequency in these signals is crucial. To determine this stimulation frequency, \n",
    "refer to the event sequence above. You'll notice there is around 1 stimulation per 100 seconds for each event. \n",
    "In the power spectrum above, you can identify a peak at approximately 0.01Hz, likely associated to this event-related hemodynamic response. You may also want to get rid of physiological oscillations: among the most important, there is the heart rate at ~1Hz and the breathing rate at ~0.3Hz. You can see the corresponding peaks in the PSD above. Based on this, the upper pass-band edge of your low pass filter can be set to 0.2Hz. You can use transbandwidth to make a smooth transition between the passband (0.2Hz) and the stopband (0.3Hz, frequency above which you will get your signal contaminated by physiological oscillations). Filters with narrow transition bandwidths are often preferred in applications where closely spaced frequency components need to be separated such as ours. However, be aware that you cannot narrow too much the transition bandwidth with low-order filters as the ones proposed by MNE library.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db449cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Now apply the filter method to retain only the hemodynamic responses from your signal\n",
    "##########\n",
    "\n",
    "#And now, let's try to filter the hemodynamic data using the filter method for raw objects \n",
    "raw_haemofiltered=raw_haemo.filter(0, 0.2, h_trans_bandwidth=0.1,\n",
    "                             l_trans_bandwidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67022520",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>💡 Pay attention! 💡</b></p>\n",
    "<p style='text-indent: 10px;'> Whenever you apply a filter, you should check that you are not removing the signal of interest. For that, make sure the task stimulation frequency is not within the frequency range of your filter ! </p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd00034-147c-46d0-80cc-c31d1e2a2332",
   "metadata": {},
   "source": [
    "Run the next cell to plot the PSD of your filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the spectrum of filtered hemodynamic data \n",
    "fig = spectrum = raw_haemofiltered.compute_psd().plot(average = True)\n",
    "fig.suptitle('After filtering', weight='bold', size='x-large',y=1.1)\n",
    "fig.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff134ca",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot time-series of hemodynamic data obtained with one channel \n",
    "raw_haemofiltered.plot(start=0,duration=200,n_channels=1,show_scrollbars=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ff740-756b-44cc-8c8b-3074c0e5dc6c",
   "metadata": {},
   "source": [
    "Congratulations ! You now have all the basics to understand and preprocess fNIRS data!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac02d6a-d896-48e0-a36c-e280f83b37df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p><b>🎉 You've reached the end of this week's notebook! Congratulations! 🎉 </b></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
