{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0f5458",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "\n",
    "Welcome to the NX-421 class! In today's week, you will get more familiar with preprocessing steps typically conducted in MRI analysis!\n",
    "\n",
    "# Lab 2: Preprocessing\n",
    "\n",
    "In this lab, we will have you look at some of the essential preprocessing steps that should be conducted before any analysis. \n",
    "\n",
    "The most important steps, you will run separately to get to know them. Some more involved steps can also be found in details in the **advanced_preprocessing.ipynb** notebook, which is **purely optional**. \n",
    "\n",
    "Once you've studied the most important steps, we will show how to conduct all these in a streamlined fashion.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#805AD5; color: #90EE90; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>‚ö†Ô∏è Preprocessing warnings ‚ö†Ô∏è</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "The steps of preprocessing you have seen in class are dependent on the analysis you wish to conduct.\n",
    "In particular, there is no yet a clear consensus on the order in which some steps should be applied, although said order is known to impact subsequent analysis.\n",
    "As an example, we will teach you how to put your subjects in a common space, the MNI space through a step of <b>normalization</b> (more on that later!), such that subjects can be compared at the group-level. Assume now that for the purpose of your analysis, you are interested only in a single subject, or that the method of choice should be at an individual level for your application. Clearly the normalization is superfluous in this case.\n",
    "You should also know that most steps will have parameters to set. These parameters can be set at the population level (this is often the case). Such choice means the preprocessing won't be optimal for each subject.\n",
    "<br>\n",
    "</p></span>\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"warning\" style='background-color:#90EE90; color: #805AD5; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>Preprocessing pipelines</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "We will teach you how to perform each step individually, so that you get a precise understanding of what each step's purpose is. You will then use software to perform these steps for you, <a href=\"https://fmriprep.org/en/stable/\">fMRIPrep</a>. \n",
    "<br>\n",
    "</p></span>\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>üí° Quality Control (QC): the 1 - 10 - 100 dollar rule üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "The 1-10-100 rule states that it takes 1 dollar to verify and correct data at the start, 10 dollars to identify and clean data after the fact and 100 dollars to correct a failure due to bad data.\n",
    "\n",
    "In preprocessing this is especially true. It will take you much less effort to first look at your data, detect what might be wrong from the start and deal with it rather than apply everything blindly and notice after the fact that something went wrong. Always, ALWAYS look at your data before <u>anything and any analysis</u>. A surgeon always looks at the patient before operating, you should do the same: you are surgeons to your dataset, so please look at it carefully (it's craving for attention, the poor thing üíî).\n",
    "These intermediary steps of controlling the health of your dataset are called quality control steps. You're doing exactly what you might expect to do: you check the quality of your data before and after a given step, to ensure that nothing went awry for example.\n",
    "<br>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c069589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose imports to handle paths, files etc\n",
    "import os\n",
    "import os.path as op\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# Useful functions to define and import datasets from open neuro\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "\n",
    "\n",
    "def reset_overlays():\n",
    "    \"\"\"\n",
    "    Clears view and completely remove visualization. All files opened in FSLeyes are closed.\n",
    "    The view (along with any color map) is reset to the regular ortho panel.\n",
    "    \"\"\"\n",
    "    l = frame.overlayList\n",
    "    while(len(l)>0):\n",
    "        del l[0]\n",
    "    frame.removeViewPanel(frame.viewPanels[0])\n",
    "    # Put back an ortho panel in our viz for future displays\n",
    "    frame.addViewPanel(OrthoPanel)\n",
    "    \n",
    "def mkdir_no_exist(path):\n",
    "    if not op.isdir(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def direct_file_download_open_neuro(file_list, file_types, dataset_id, dataset_version, save_dirs):\n",
    "    # https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:sub-001_scans.tsv\n",
    "    for i, n in enumerate(file_list):\n",
    "        subject = n.split('_')[0]\n",
    "        download_link = 'https://openneuro.org/crn/datasets/{}/snapshots/{}/files/{}:{}:{}'.format(dataset_id, dataset_version, subject, file_types[i],n)\n",
    "        print('Attempting download from ', download_link)\n",
    "        download_url(download_link, op.join(save_dirs[i], n))\n",
    "        print('Ok')\n",
    "        \n",
    "def get_json_from_file(fname):\n",
    "    f = open(fname)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac62a0e",
   "metadata": {},
   "source": [
    "## 0. Loading a dataset\n",
    "\n",
    "We have not touched yet upon how you might load datasets. \n",
    "\n",
    "### BIDS standard\n",
    "For this, we should first tell you more about a standard: the BIDS standard.\n",
    "This one allows you to format a dataset such that other researchers in neuroimaging can reuse your data with the smallest overhead possible. It is a way to unify how files and acquisitions are organized in folders.\n",
    "\n",
    "This unification comes with several advantages including many tools that ease our life.\n",
    "Indeed, if your dataset is in such a format, it is very easy to conduct any analysis: your scripts will expect a specific structure, so you don't need to play a million times with paths for example!\n",
    "\n",
    "A whole software ecosystem has evolved around this standard, including tools that enable us to load datasets that are BIDs compliant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43aa685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'ds004226'\n",
    "subject = '001' \n",
    "\n",
    "# Download one subject's data from each dataset\n",
    "sample_path = \"dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(os.path.abspath(\"\"),sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "mkdir_no_exist(bids_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed541d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"\"\"openneuro-py download --dataset {} --include sub-{}/anat/* \n",
    "          --include sub-{}/func/sub-{}_task-sitrep_run-01_bold.nii.gz \n",
    "          --include sub-{}/func/sub-{}_task-sitrep_run-01_bold.json \n",
    "          --target_dir {}\"\"\".format(dataset_id, subject, subject, subject, subject, subject, bids_root).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944c505",
   "metadata": {},
   "source": [
    "Have a look at the terminal in which you launched this notebook. You will see an output that looks like this:\n",
    "<img src=\"imgs/download_picture.png\"/>\n",
    "\n",
    "What this tells you is that the download is, actually, still undergoing. Be mindful of this when downloading a dataset: you should avoid opening a file until it is fully downloaded, otherwise you have a high chance of corrupting it!\n",
    "\n",
    "Luckily here, we have a blocking command above which will only return when the download is finished. Always check that your commands are blocking and design your code accordingly!\n",
    "\n",
    "We rely above on openneuro-py to pull our files. We provide you with the way below to download  directly the files as well, should you wish to pull files from open-neuro without openneuro-py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf69bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download from  https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:func:sub-001_task-sitrep_run-01_bold.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub-001:func:sub-001_task-sitrep_run-01_bold.nii.gz: 169MB [02:20, 1.20MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "Attempting download from  https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:func:sub-001_task-sitrep_run-01_bold.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub-001:func:sub-001_task-sitrep_run-01_bold.json: 8.19kB [00:00, 11.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "Attempting download from  https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:anat:sub-001_T1w.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub-001:anat:sub-001_T1w.nii.gz: 14.3MB [00:15, 943kB/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "Attempting download from  https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:anat:sub-001_T1w.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub-001:anat:sub-001_T1w.json: 8.19kB [00:00, 11.9kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "func_path = op.join(bids_root, 'sub-001', 'func')\n",
    "anat_path = op.join(bids_root, 'sub-001', 'anat')\n",
    "mkdir_no_exist(op.join(bids_root, 'sub-001'))\n",
    "mkdir_no_exist(func_path)\n",
    "mkdir_no_exist(anat_path)\n",
    "\n",
    "direct_file_download_open_neuro(file_list=['sub-001_task-sitrep_run-01_bold.nii.gz', \n",
    "                                           'sub-001_task-sitrep_run-01_bold.json',\n",
    "                                           'sub-001_T1w.nii.gz',\n",
    "                                           'sub-001_T1w.json'], \n",
    "                                file_types=['func', 'func', 'anat', 'anat'], \n",
    "                                dataset_id=dataset_id, \n",
    "                                dataset_version='1.0.0', \n",
    "                                save_dirs=[func_path,\n",
    "                                           func_path,\n",
    "                                           anat_path,\n",
    "                                           anat_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d578d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_no_exist(op.join(bids_root, 'derivatives'))\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'anat'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'func'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-001', 'fmap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1091c",
   "metadata": {},
   "source": [
    "Once the download is done, we can have a look at the resulting folder structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac6ea4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ds004226/\n",
      "|--- CHANGES\n",
      "|--- dataset_description.json\n",
      "|--- participants.tsv\n",
      "|--- derivatives/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-001/\n",
      "|------------ anat/\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-001/\n",
      "|------ anat/\n",
      "|--------- sub-001_T1w.json\n",
      "|--------- sub-001_T1w.nii.gz\n",
      "|------ func/\n",
      "|--------- sub-001_task-sitrep_run-01_bold.json\n",
      "|--------- sub-001_task-sitrep_run-01_bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62bc37",
   "metadata": {},
   "source": [
    "This organization is typical of a BIDs dataset. Each subject's file is split between anatomical data and functional data. You are already a bit familiar with the .nii.gz file extension, but what might be the .json file? Well, let's open it to figure it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46573233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Modality': 'MR',\n",
       " 'MagneticFieldStrength': 3,\n",
       " 'Manufacturer': 'Siemens',\n",
       " 'ManufacturersModelName': 'Skyra',\n",
       " 'InstitutionName': 'Princeton_University_-_Neuroscience_Institute',\n",
       " 'InstitutionalDepartmentName': 'Department',\n",
       " 'InstitutionAddress': 'Washington_and_Faculty_Rd._-_Building_25_25_Princeton_NJ_US_085',\n",
       " 'DeviceSerialNumber': '45031',\n",
       " 'StationName': 'AWP45031',\n",
       " 'BodyPartExamined': 'BRAIN',\n",
       " 'PatientPosition': 'HFS',\n",
       " 'ProcedureStepDescription': 'TamirL_Mark',\n",
       " 'SoftwareVersions': 'syngo_MR_E11',\n",
       " 'MRAcquisitionType': '2D',\n",
       " 'SeriesDescription': 'EPI_2.5mm_1.5TR_32TE_SMS4_Siemens',\n",
       " 'ProtocolName': 'EPI_2.5mm_1.5TR_32TE_SMS4_Siemens',\n",
       " 'ScanningSequence': 'EP',\n",
       " 'SequenceVariant': 'SK',\n",
       " 'ScanOptions': 'FS',\n",
       " 'SequenceName': '_epfid2d1_78',\n",
       " 'ImageType': ['ORIGINAL', 'PRIMARY', 'M', 'ND', 'NORM', 'MOSAIC'],\n",
       " 'SeriesNumber': 9,\n",
       " 'AcquisitionTime': '16:17:36.350000',\n",
       " 'AcquisitionNumber': 1,\n",
       " 'SliceThickness': 2.5,\n",
       " 'SpacingBetweenSlices': 2.5,\n",
       " 'SAR': 0.0635751,\n",
       " 'EchoTime': 0.032,\n",
       " 'RepetitionTime': 1.5,\n",
       " 'FlipAngle': 70,\n",
       " 'PartialFourier': 1,\n",
       " 'BaseResolution': 78,\n",
       " 'ShimSetting': [3407, 8775, 9441, 650, 135, 271, -116, 348],\n",
       " 'TxRefAmp': 300.048,\n",
       " 'PhaseResolution': 1,\n",
       " 'ReceiveCoilName': 'HeadNeck_64',\n",
       " 'ReceiveCoilActiveElements': 'HC1-7',\n",
       " 'PulseSequenceDetails': '%SiemensSeq%_ep2d_bold',\n",
       " 'ConsistencyInfo': 'N4_VE11C_LATEST_20160120',\n",
       " 'MultibandAccelerationFactor': 4,\n",
       " 'PercentPhaseFOV': 100,\n",
       " 'EchoTrainLength': 78,\n",
       " 'PhaseEncodingSteps': 78,\n",
       " 'AcquisitionMatrixPE': 78,\n",
       " 'ReconMatrixPE': 78,\n",
       " 'BandwidthPerPixelPhaseEncode': 20.678,\n",
       " 'EffectiveEchoSpacing': 0.000620007,\n",
       " 'DerivedVendorReportedEchoSpacing': 0.000620007,\n",
       " 'TotalReadoutTime': 0.0477406,\n",
       " 'PixelBandwidth': 2005,\n",
       " 'DwellTime': 3.2e-06,\n",
       " 'PhaseEncodingDirection': 'j-',\n",
       " 'SliceTiming': [0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365,\n",
       "  0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365,\n",
       "  0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365,\n",
       "  0.6825,\n",
       "  0,\n",
       "  0.795,\n",
       "  0.1125,\n",
       "  0.91,\n",
       "  0.2275,\n",
       "  1.025,\n",
       "  0.34,\n",
       "  1.1375,\n",
       "  0.455,\n",
       "  1.2525,\n",
       "  0.5675,\n",
       "  1.365],\n",
       " 'ImageOrientationPatientDICOM': [0.999593,\n",
       "  0.0224594,\n",
       "  0.0175892,\n",
       "  -0.0207639,\n",
       "  0.99561,\n",
       "  -0.0912658],\n",
       " 'InPlanePhaseEncodingDirectionDICOM': 'COL',\n",
       " 'ConversionSoftware': 'dcm2niix',\n",
       " 'ConversionSoftwareVersion': 'v1.0.20171215 GCC4.8.5'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_json_from_file(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53284c",
   "metadata": {},
   "source": [
    "This JSON is extremely important: it is what we call a JSON **sidecar**, and it holds precious acquisition informations! Based **only on the text printed above**, are you able to determine any or all of the following?\n",
    "- [ ] TR?\n",
    "- [ ] Modality of acquisition?\n",
    "- [ ] How many Teslas the scanner was?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5574168",
   "metadata": {},
   "source": [
    "### Loading more datasets: how to\n",
    "\n",
    "Great! Note that we've loaded only one subject and one file of each modality for the subject. You can have a look <a href=\"https://openneuro.org/datasets/ds004226/versions/1.0.0\">here</a> for this dataset. As you can see, it is a big dataset; we've restricted our download to the bare minimum to spare your computer's disk space as much as possible.\n",
    "\n",
    "Notice two things on the web page. The first is the dataset's accession number:\n",
    "<img src=\"imgs/openneuro_access_nbr.png\">\n",
    "This number is the one we've put in our code earlier, to specify what dataset we wanted to load from:\n",
    "```python\n",
    "dataset_fmap = 'ds004226'\n",
    "...\n",
    "openneuro.download(dataset=dataset_fmap, ...)\n",
    "\n",
    "```\n",
    "\n",
    "Should you wish to download another openneuro dataset that piqued your interest, you'll simply need to change the above variable with the accession number of the dataset on the corresponding page!\n",
    "\n",
    "Secondly, you can observe the entire folder structure, size and many other interest informations of this dataset by simply scrolling its page!\n",
    "<img src=\"imgs/openneuro_full_view.png\">\n",
    "You can really decide whether a dataset is what you need this way, before burdening your connection with any heavy download :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5c9fe",
   "metadata": {},
   "source": [
    "## 0.5 A mysterious command to run\n",
    "\n",
    "Before going any further, open a terminal and launch the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cd346fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docker run -ti --rm -v /home/guibert/NX_NSSP/Week2/dataset/ds004226:/data:ro -v /home/guibert/NX_NSSP/Week2/dataset/ds004226/derivatives:/out -v /home/NX421:/license_path:ro nipreps/fmriprep:latest /data /out/fmiprep_results participant --participant-label 001 --fs-license-file /license_path/license.txt'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"docker run -ti --rm -v {}:/data:ro -v {}:/out -v /home/NX421:/license_path:ro nipreps/fmriprep:latest /data /out/fmiprep_results participant --participant-label {} --fs-license-file /license_path/license.txt\".format(bids_root, \n",
    "                                                                                                                                               deriv_root, \"001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d978b",
   "metadata": {},
   "source": [
    "It will run in the background and we will come back to it later !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abf645",
   "metadata": {},
   "source": [
    "## 1. Anatomical preprocessing\n",
    "\n",
    "Let's have a look at the nice anatomical we downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95a7d85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(sub-001_T1w, /home/guibert/NX_NSSP/Week2/dataset/ds004226/sub-001/anat/sub-001_T1w.nii.gz)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_overlays()\n",
    "load(op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12ca9e",
   "metadata": {},
   "source": [
    "Take some time to explore the exquisite anatomy of the brain. Notice around the brain, the human skull. It is full of regions which show up **in this contrast** whiter than others. Based on what you might know from class about the T1 contrast, can you identify the different regions annotated below taken from another T1?\n",
    "\n",
    "As a hint, think about white matter: it is composed of axons and myelin. Contrast with grey matter, which is comprised of soma. Based on your understanding:\n",
    "<img src=\"imgs/annotated_regions.png\">\n",
    "\n",
    "\n",
    "- [ ] Tissues high in fat are bright in T1 contrast, which is why white matter is brighter than grey matter\n",
    "- [ ] Tissues high in fibers are bright in T1 contrast, which is why white matter is brighter than grey matter\n",
    "- [ ] Region 1 is likely high in fibers and might be tendons and ligaments, which are dense connective tissues full of fibers.\n",
    "- [ ] Region 1 is likely high in fat, and is probably subcutaneous fat.\n",
    "- [ ] Region 2 contains a mix of fat and water, hence the slightly darker color. Given its location, it is probably bone marrow.\n",
    "- [ ] Region 2 contains a mix of fibers and water, hence the slightly darker color. Given its location, it is probably the dura mater, connective tissues that make up the outer-most layer of the meninges.\n",
    "- [ ] Region 3 contains air, which is why we do not see it in T1.\n",
    "- [ ] Region 3 contains mostly water, which appears dark in T1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ca769",
   "metadata": {},
   "source": [
    "### Skull stripping\n",
    "\n",
    "#### Preprocessing and BIDs\n",
    "An important part of **anatomical** preprocessing is to remove the skull around the brain.\n",
    "To adhere to the BIDs format, all modified files should be put in a new folder, called derivatives, such that you always have clean data in the source directory. The derivatives folder can be used for different preprocessing and treatments, each needing their own subfolders. In our case, we've created a single folder, preprocessed_data, hence the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c56df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ds004226/\n",
      "|--- CHANGES\n",
      "|--- dataset_description.json\n",
      "|--- participants.tsv\n",
      "|--- derivatives/\n",
      "|------ fmiprep_results/\n",
      "|--------- logs/\n",
      "|------------ CITATION.bib\n",
      "|------------ CITATION.html\n",
      "|------------ CITATION.md\n",
      "|------------ CITATION.tex\n",
      "|--------- sourcedata/\n",
      "|------------ freesurfer/\n",
      "|--------- sub-001/\n",
      "|------------ figures/\n",
      "|------------ func/\n",
      "|------------ log/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-001/\n",
      "|------------ anat/\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-001/\n",
      "|------ anat/\n",
      "|--------- sub-001_T1w.json\n",
      "|--------- sub-001_T1w.nii.gz\n",
      "|------ func/\n",
      "|--------- sub-001_task-sitrep_run-01_bold.json\n",
      "|--------- sub-001_task-sitrep_run-01_bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b113b67",
   "metadata": {},
   "source": [
    "#### Actual skull stripping\n",
    "\n",
    "Perfect! Let's move on to actually extracting the brain! To make it easier for you to detect what was actually extracted, we will let the brain extraction proceed, using FSL's <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET/UserGuide\">BET</a> (brain extraction tool) and show you the mask of the region determined by FSL as brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83a7f933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/guibert/NX_NSSP/Week2/dataset/ds004226/derivatives/preprocessed_data'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66ef5c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anatomical_path = op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz')\n",
    "betted_brain_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w')\n",
    "resulting_mask_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_mask')\n",
    "bet(anatomical_path, betted_brain_path, mask=resulting_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6738408a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(sub-001_T1w_mask, /home/guibert/NX_NSSP/Week2/dataset/ds004226/derivatives/preprocessed_data/sub-001/anat/sub-001_T1w_mask.nii.gz)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c91a6",
   "metadata": {},
   "source": [
    "Is the mask nicely fitting around the brain? What you would like is that the mask is taking all parts of the brain and excluding the rest.\n",
    "To answer this one, play with the mask's opacity in FSL eyes.<br>\n",
    "*Hint: have a look at the frontal regionsm inspect as well the superior parietal regions*<br>\n",
    "<br><br>\n",
    "What you are doing here is simply **Quality Control** (QC). It is a crucial step that you should **NEVER** skip when dealing with data preprocessing. As all steps are dependent on the success of previous steps, always make sure that everything is performing properly before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d4348",
   "metadata": {},
   "source": [
    "#### Improving the fit\n",
    "If you look a bit into bet's documentation, you'll quickly find that there are parameters with which you can play; robust brain centre estimation and fractional intensity threshold. To demonstrate the importance and impact of these parameters, let's use a robust brain center estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d66735d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet(anatomical_path, betted_brain_path, mask=resulting_mask_path, robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "361984c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(sub-001_T1w_mask, /home/guibert/NX_NSSP/Week2/dataset/ds004226/derivatives/preprocessed_data/sub-001/anat/sub-001_T1w_mask.nii.gz)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_overlays()\n",
    "\n",
    "load(op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz'))\n",
    "load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4141db1",
   "metadata": {},
   "source": [
    "How good is the mask now?\n",
    "\n",
    "Is it perfect? (*Hint: have a look at voxels around 94-209-131*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4679f",
   "metadata": {},
   "source": [
    "#### Hand corrections\n",
    "If you really want good fit, you might want to resort to **hand correcting the mask**. \n",
    "\n",
    "FSLeyes readily allows you to do such things! While on FSLeyes, press **Alt + E** to open the editing interface.\n",
    "<img src=\"imgs/editing_menu_fsl.png\">\n",
    "<center><i>FSLeyes editing menu</i></center>\n",
    "\n",
    "We will work here on removing some unwanted voxels. Toggle the 'Select mode' first. This way, FSL will show us which voxels we currently have selected, before changing their value\n",
    "<img src=\"imgs/selection_mode_toggle_fsl.png\">\n",
    "<center><i>Make sure to be in Select mode by clicking it</i></center>\n",
    "\n",
    "Then let's pick the pencil tool, to select the voxels we want.\n",
    "<img src=\"imgs/brush_tool.png\">\n",
    "\n",
    "Good, we're set and we can now select voxels. We'll try to select some **unwanted** voxels. Simply paint over them!\n",
    "<img src=\"imgs/painted_voxels.png\">\n",
    "<center><i>Selected voxels are shown in purple</i></center>\n",
    "\n",
    "Now, we are dealing with a mask. We thus want to put the value of our selection to **0**, so as to remove it from the mask. To do so, we must change the fill value to 0, and then click to replace our selection with the provided value:\n",
    "<img src=\"imgs/paint_steps.png\">\n",
    "<center><i>The two steps to set selection to a specific fill value.</i></center>\n",
    "<img src=\"imgs/mask_painted.png\">\n",
    "<center><i>Painting with zero a mask means we remove the painted voxels from the mask.</i></center>\n",
    "\n",
    "It remains now to apply the mask to our anatomical data. This is fortunately something that you now know how to do from the previous lab! Fill in the next cell with the appropriate code **and make sure to save the masked brain in the proper directory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd046f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill me with the code to use your mask and apply it to the subject's anatomical data. \n",
    "# Save the result in the derivatives folder!!\n",
    "??? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda132c",
   "metadata": {},
   "source": [
    "### Tissue segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b2abc",
   "metadata": {},
   "source": [
    "For the purpose of analysis, it can be useful to separate the tissues into tissue classes; in particular extracting the white matter, grey matter and cerebrospinal fluid (abreviated as CSF) is very interesting in fMRI analysis. Consider for example an analysis that you wish to restrict to the somas of your neurons, would it make sense to conduct your analysis on the CSF ?\n",
    "\n",
    "You'll find that the segmentation is not done on fMRI volumes; it is done on the anatomical and the resulting tissue masks are then used on the functional data. Can you imagine why this is the case?\n",
    "\n",
    "Let's perform tissue segmentation. To do so, we'll use FSL's <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST\">FAST</a> (FMRIB's Automated Segmentation Tool).\n",
    "\n",
    "The underlying idea of FAST is to try and model each voxel's intensity as being a mixture between the different tissue types.\n",
    "Pay attention in the documentation to the following line:\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b></b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Before running FAST an image of a head should first be brain-extracted, using BET. The resulting brain-only image can then be fed into FAST.</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "Based on this, **in the cell below choose which image should be used as fast_target, between the anatomical_path and the brain_extracted_path images**.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:orange; color: #112A46; border-left: solid red 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üêû Troubleshooting: FSL stopped responding </b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    It is perfectly possible (even likely) that FSLeyes will stop responding over the course of this lab. This is perfectly normal! Simply wait for whichever function (such as FAST) to finish and it should start responding again, don't worry too quickly, be patient :)</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4be9e751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anatomical_path = op.join(bids_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz')\n",
    "bet_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w')\n",
    "\n",
    "fast_target = bet_path # Replace with either anatomical_path or bet_path (note: you can try both and decide which is more reasonable!)\n",
    "\n",
    "[os.remove(f) for f in glob.glob(op.join(preproc_root, 'sub-001', 'anat', '*fast*'))] # Just to clean the directory in between runs of the cell\n",
    "segmentation_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w_fast')\n",
    "fast(imgs=[fast_target], out=segmentation_path, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525450b2",
   "metadata": {},
   "source": [
    "Let's check the quality of the segmentation, shall we?\n",
    "We want to extract 3 tissue types here: the white matter, the grey matter and the csf. How well did fast perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50f90ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(sub-001_T1w, /home/guibert/NX_NSSP/Week2/dataset/ds004226/derivatives/preprocessed_data/sub-001/anat/sub-001_T1w.nii.gz)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_overlays()\n",
    "load(bet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd7eb6",
   "metadata": {},
   "source": [
    "If you look at the directories now, we have new files in our hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa0fd6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ds004226/\n",
      "|--- CHANGES\n",
      "|--- dataset_description.json\n",
      "|--- participants.tsv\n",
      "|--- derivatives/\n",
      "|------ fmiprep_results/\n",
      "|--------- logs/\n",
      "|------------ CITATION.bib\n",
      "|------------ CITATION.html\n",
      "|------------ CITATION.md\n",
      "|------------ CITATION.tex\n",
      "|--------- sourcedata/\n",
      "|------------ freesurfer/\n",
      "|--------------- fsaverage/\n",
      "|--------------- sub-001/\n",
      "|--------- sub-001/\n",
      "|------------ figures/\n",
      "|--------------- sub-001_desc-about_T1w.html\n",
      "|--------------- sub-001_desc-conform_T1w.html\n",
      "|--------------- sub-001_desc-summary_T1w.html\n",
      "|--------------- sub-001_task-sitrep_run-01_desc-validation_bold.html\n",
      "|------------ func/\n",
      "|--------------- sub-001_task-sitrep_run-01_from-scanner_to-boldref_mode-image_xfm.txt\n",
      "|------------ log/\n",
      "|--------------- 20230925-093603_45554640-44ae-4e5f-9e36-f3033f9c69e0/\n",
      "|------ preprocessed_data/\n",
      "|--------- sub-001/\n",
      "|------------ anat/\n",
      "|--------------- sub-001_T1w.nii.gz\n",
      "|--------------- sub-001_T1w_fast_mixeltype.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pve_0.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pve_1.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pve_2.nii.gz\n",
      "|--------------- sub-001_T1w_fast_pveseg.nii.gz\n",
      "|--------------- sub-001_T1w_fast_seg.nii.gz\n",
      "|--------------- sub-001_T1w_mask.nii.gz\n",
      "|------------ fmap/\n",
      "|------------ func/\n",
      "|--- sub-001/\n",
      "|------ anat/\n",
      "|--------- sub-001_T1w.json\n",
      "|--------- sub-001_T1w.nii.gz\n",
      "|------ func/\n",
      "|--------- sub-001_task-sitrep_run-01_bold.json\n",
      "|--------- sub-001_task-sitrep_run-01_bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e1ac9",
   "metadata": {},
   "source": [
    "The pve files correspond to our segmented tissues. We have exactly three files, because we set n_classes to 3 above:\n",
    "```python\n",
    "fast(..., n_classes=3)\n",
    "```\n",
    "\n",
    "Let's try to identify which segmentation is which tissue type in the brain. To do this, you'll have to visualize the tissues and decide for yourself:\n",
    "\n",
    "- [ ] pve_0 is white matter, pve_1 is grey matter, pve_2 is CSF\n",
    "- [ ] pve_0 is grey matter, pve_1 is white matter, pve_2 is CSF\n",
    "- [ ] pve_0 is grey matter, pve_1 is CSF, pve_2 is white matter\n",
    "- [ ] pve_0 is CSF, pve_1 is grey matter, pve_2 is white matter\n",
    "\n",
    "\n",
    "To make it easier on you, we will display:\n",
    "\n",
    "- pve_0 in <span style=\"color:red;\">red</span>\n",
    "- pve_1 in <span style=\"color:green;\">green</span>\n",
    "- pve_2 in <span style=\"color:blue;\">blue</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29c1afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(glob.glob(op.join(preproc_root, 'sub-001', 'anat','*pve_0*'))[0])\n",
    "load(glob.glob(op.join(preproc_root, 'sub-001', 'anat','*pve_1*'))[0])\n",
    "load(glob.glob(op.join(preproc_root, 'sub-001', 'anat','*pve_2*'))[0])\n",
    "displayCtx.getOpts(overlayList[1]).cmap = 'Red'\n",
    "displayCtx.getOpts(overlayList[2]).cmap = 'Green'\n",
    "displayCtx.getOpts(overlayList[3]).cmap = 'Blue'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbcd4d",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üè† Tissues and contrast: Take home message üè†</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Tissues in T1 or T2 will show up in different colour, dependent on their content. This is because their content affects their relaxation time and, in turn, the intensity captured during the acquisition. Here are different tissue types for both modalities (from <a href=\"https://www.researchgate.net/publication/324396120_Basic_MRI_for_the_liver_oncologists_and_surgeons\">Vu, Lan N., John N. Morelli, and Janio Szklaruk. \"Basic MRI for the liver oncologists and surgeons.\" Journal of hepatocellular carcinoma 5 (2017): 37.</a>):\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>MR</th>\n",
    "            <th>High signal (bright)</th>\n",
    "            <th>Low signal (dark)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>T1</th>\n",
    "            <th>Fat, melanin</th>\n",
    "            <th>Iron</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Blood</th>\n",
    "            <th>Water</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Proteinaceous fluid</th>\n",
    "            <th>Air, bone</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Paramagnetic substances</th>\n",
    "            <th>Collagen</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Chelated gadolinium contrast</th>\n",
    "            <th>Most tumors</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>T2</th>\n",
    "            <th>Water</th>\n",
    "            <th>Air</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Edema</th>\n",
    "            <th>Bone</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Blood</th>\n",
    "            <th>Hemosiderin, deoxyhemoglobin, methemoglobin</th>\n",
    "        </tr>\n",
    "    </table>\n",
    "    Typical preprocessing steps of anatomical data starts by extracting the brain by removing skull tissues. This step can be conducted mostly automatically, but it is perfectly possible to manually correct the extracted brain, either to include more or less voxels when tweaking the parameters does not yield satisfactory results.\n",
    "    The extracted brain can be segmented into different tissues. Using the difference in brightness due to contrast, we can separate the grey matter, the white matter and the CSF, which is useful for later analysis.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6483d46-93dd-4709-952d-16d3f3d134d4",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "This is a step you already conducted last week, if you remember! If not, go back to lab 1 and look for Ducky and his sunglasses. It is also critical to proper preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f84987",
   "metadata": {},
   "source": [
    "## Anatomical: conclusions\n",
    "\n",
    "As a final note, all these steps (<u>including</u> non linear normalization!) can be done automatically for you with a single command: fsl_anat. So you might want to use this command, instead of running all of the above.\n",
    "Here's a quick run for you (note: it will take several minutes to complete so be patient :) ).\n",
    "\n",
    "Now, there are some subtleties and renamings that are needed because of the way FSL operates. We thus provide you with a wrapper around fsl_anat to do all this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "475523a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def fsl_anat_wrapped(anatomical_target, output_path):\n",
    "    fsl_anat(img=anatomical_target, clobber=True, nosubcortseg=True, o=output_path)\n",
    "    # Now move all files from the output_path.anat folder created by FSL to \n",
    "    # the actual output_path\n",
    "    fsl_anat_path = output_path+'.anat'\n",
    "    files_to_move = glob.glob(op.join(fsl_anat_path, '*'))\n",
    "    for f in files_to_move:\n",
    "        shutil.move(f, op.join(output_path, op.split(f)[1]))\n",
    "    \n",
    "    # Remove the output_path.anat folder\n",
    "    os.rmdir(fsl_anat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 25 11:48:41 CEST 2023\n",
      "Reorienting to standard orientation\n",
      "Mon Sep 25 11:48:53 CEST 2023\n",
      "Automatically cropping the image\n",
      "Starting Single Image Segmentation\n",
      "T1-weighted image\n",
      "Imagesize : 176 x 256 x 170\n",
      "Pixelsize : 1 x 1 x 1\n",
      "\n",
      "1 6.64379\n",
      "2 6.9344\n",
      "3 7.23706\n",
      "KMeans Iteration 0\n",
      "KMeans Iteration 1\n",
      "KMeans Iteration 2\n",
      "KMeans Iteration 3\n",
      "KMeans Iteration 4\n",
      "KMeans Iteration 5\n",
      "KMeans Iteration 6\n",
      "KMeans Iteration 7\n",
      "KMeans Iteration 8\n",
      "KMeans Iteration 9\n",
      "KMeans Iteration 10\n",
      "KMeans Iteration 11\n",
      "KMeans Iteration 12\n",
      "KMeans Iteration 13\n",
      "KMeans Iteration 14\n",
      "Tanaka Iteration 0 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.73111e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.75433e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.75483e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.75484e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.75484e+07 beta=0.02\n",
      " CLASS 1 MEAN 481.122 STDDEV 436.353 CLASS 2 MEAN 945.703 STDDEV 159.348 CLASS 3 MEAN 1458.92 STDDEV 157.875\n",
      "Tanaka Iteration 1 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.76529e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.78678e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.78721e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.78722e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.78722e+07 beta=0.02\n",
      " CLASS 1 MEAN 465.856 STDDEV 408.051 CLASS 2 MEAN 951.025 STDDEV 149.986 CLASS 3 MEAN 1416.51 STDDEV 119.989\n",
      "Tanaka Iteration 2 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.77916e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.79957e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.79997e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.79998e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.79998e+07 beta=0.02\n",
      " CLASS 1 MEAN 461.466 STDDEV 400.176 CLASS 2 MEAN 953.434 STDDEV 142.426 CLASS 3 MEAN 1413.91 STDDEV 108.362\n",
      "Tanaka Iteration 3 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.78501e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.80493e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80531e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.80532e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.80532e+07 beta=0.02\n",
      " CLASS 1 MEAN 461.526 STDDEV 398.301 CLASS 2 MEAN 956.353 STDDEV 139.154 CLASS 3 MEAN 1416.87 STDDEV 103.14\n",
      "Tanaka Iteration 4 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.7873e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.80703e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80742e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.80743e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.80743e+07 beta=0.02\n",
      " CLASS 1 MEAN 462.992 STDDEV 398.347 CLASS 2 MEAN 959.298 STDDEV 138.097 CLASS 3 MEAN 1420.22 STDDEV 100.148\n",
      "Tanaka Iteration 5 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.78813e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.80778e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80816e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.80817e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.80817e+07 beta=0.02\n",
      " CLASS 1 MEAN 464.522 STDDEV 398.881 CLASS 2 MEAN 961.852 STDDEV 138.065 CLASS 3 MEAN 1423.08 STDDEV 98.1301\n",
      "Tanaka Iteration 6 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.78831e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.80795e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80833e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.80834e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.80834e+07 beta=0.02\n",
      " CLASS 1 MEAN 465.749 STDDEV 399.445 CLASS 2 MEAN 963.92 STDDEV 138.47 CLASS 3 MEAN 1425.42 STDDEV 96.6314\n",
      "Tanaka Iteration 7 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.78818e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.80784e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80822e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.80823e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.80823e+07 beta=0.02\n",
      " CLASS 1 MEAN 466.667 STDDEV 399.938 CLASS 2 MEAN 965.554 STDDEV 139.054 CLASS 3 MEAN 1427.3 STDDEV 95.4521\n",
      "Tanaka Iteration 8 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.7879e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.80759e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80798e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.80799e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.80799e+07 beta=0.02\n",
      " CLASS 1 MEAN 467.347 STDDEV 400.357 CLASS 2 MEAN 966.839 STDDEV 139.688 CLASS 3 MEAN 1428.82 STDDEV 94.4929\n",
      "Tanaka Iteration 9 bias field 10\n",
      "Tanaka-inner-loop-iteration=0 MRFWeightsTotal=1.78758e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=1 MRFWeightsTotal=1.8073e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=2 MRFWeightsTotal=1.80769e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=3 MRFWeightsTotal=1.8077e+07 beta=0.02\n",
      "Tanaka-inner-loop-iteration=4 MRFWeightsTotal=1.8077e+07 beta=0.02\n",
      " CLASS 1 MEAN 467.86 STDDEV 400.713 CLASS 2 MEAN 967.855 STDDEV 140.293 CLASS 3 MEAN 1430.06 STDDEV 93.7564\n",
      "\n",
      " tissue 1 509235\n",
      " tissue 2 731452\n",
      " tissue 3 580452\n",
      " total tissue 1.82114e+06\n",
      "Mon Sep 25 11:53:46 CEST 2023\n",
      "Extrapolating bias field from central region\n",
      "Cost is 2.53793e-06\n",
      "Cost is 3.45478e-07\n",
      "Mon Sep 25 11:54:16 CEST 2023\n",
      "Registering to standard space (linear)\n",
      "Mon Sep 25 11:54:45 CEST 2023\n",
      "Registering to standard space (non-linear)\n",
      "Mon Sep 25 12:00:12 CEST 2023\n",
      "Performing brain extraction (using FNIRT)\n"
     ]
    }
   ],
   "source": [
    "fsl_anat_wrapped(anatomical_path, op.join(preproc_root, 'sub-001', 'anat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02164173",
   "metadata": {},
   "source": [
    "Let's inspect the resulting files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ff2eb",
   "metadata": {},
   "source": [
    "That's a lot of files! But let's worry about mostly two of them. <br>\n",
    "Notice the T1_to_MNI_lin and the T1_to_MNI_nonlin ?\n",
    "<br>In the former's case, FLIRT was run to obtain a linear normalization, whereas FNIRT was used for the latter to obtain a non linear normalization. But what difference does it make, in practice? Well, let's inspect the results, shall we?\n",
    "\n",
    "*Hint: consider the brain landmarks, such as the ventricles but also the overall shape of the brain to determine if there was a change and if so which one(s)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load(reference)\n",
    "load(op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_lin'))\n",
    "load(op.join(preproc_root, 'sub-001', 'anat', 'T1_to_MNI_nonlin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726e64d",
   "metadata": {},
   "source": [
    "## 2. fMRI preprocessing\n",
    "\n",
    "You are now familiar with the few steps of preprocessing revolving around the T1 anatomical file. The main preprocessing starts now, with the functional data. \n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>üí° Do not forget QC! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    As always, in all your steps visualize the effect of what you're doing. This is the easiest way to check that what you're doing is actually having an effect and better yet: a *correct* effect!</p>\n",
    "</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "### 2.0 Problematic volumes removal\n",
    "\n",
    "A problem can arise in fMRI. To showcase this, please execute the cell below (we'll show you something from another dataset just to drive our point home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_root_demo = op.join(op.dirname(sample.data_path()), dataset_demo)\n",
    "op.join(bids_root_demo, 'derivatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_demo = 'ds004218'\n",
    "subject_demo = '01'\n",
    "\n",
    "# Download one subject's data from each dataset\n",
    "bids_root_demo = op.join(op.dirname(sample.data_path()), dataset_demo)\n",
    "preproc_root_demo = op.join(bids_root_demo, 'derivatives')\n",
    "\n",
    "mkdir_no_exist(bids_root_demo)\n",
    "mkdir_no_exist(op.join(bids_root_demo, 'sub-01'))\n",
    "mkdir_no_exist(op.join(bids_root_demo, 'sub-01', 'func'))\n",
    "\n",
    "\n",
    "mkdir_no_exist(preproc_root_demo)\n",
    "mkdir_no_exist(op.join(preproc_root_demo, 'sub-01'))\n",
    "mkdir_no_exist(op.join(preproc_root_demo, 'sub-01', 'anat'))\n",
    "\n",
    "os.system(\"\"\"openneuro-py download --dataset {}\n",
    "          --include sub-{}/func/sub-{}_task-listening_run-1_bold.nii.gz\n",
    "          --target_dir {}\"\"\".format(dataset_demo, subject_demo, \n",
    "                                    subject_demo, bids_root_demo).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f6baa",
   "metadata": {},
   "source": [
    "We've downloaded one functional volume from another dataset, because the phenomenon is really visible in this dataset. \n",
    "Before going any further in this tutorial, let's open up our data and have a look at them. <u>You should always look at your data before conducting any sort of analysis</u>. See if you find anything at all that looks strange. You should look for\n",
    "\n",
    "- [ ] Volumes moving in space (ie: head motion)\n",
    "- [ ] Non homogeneities that do not seem to be coming from brain activity\n",
    "\n",
    "To open the volume of interest in FSL eyes, simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a365c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(op.join(bids_root_demo, 'sub-01', 'func', 'sub-01_task-listening_run-1_bold.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f425046",
   "metadata": {},
   "source": [
    "Did you find anything?\n",
    "If so, what volumes would you remove, approximately?\n",
    "\n",
    "#### 2.0.1 Field stabilization\n",
    "\n",
    "The scanner's field takes some time to settle. You probably noticed that the volumes had an initially high contrast that quickly decayed to some baseline? It is precisely caused by the scanner's field settling.\n",
    "\n",
    "There's little to be done in this regard; we can only throw away the volumes that are contaminated in this specific case, as the 'staircase' brain that we observe is not really meaningful and might hurt our analysis later on.\n",
    "\n",
    "We'll throw away the first 10 volumes (first 20 seconds here), to err on the safe side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_realign = glob.glob(op.join(bids_root_demo, 'sub-01', 'func', 'sub-01_task-listening_run-1_bold.nii*'))[0]\n",
    "output_target = op.join(preproc_root_demo, 'sub-01', 'func', 'sub-01_task-listening_run-1_bold_settled')\n",
    "\n",
    "# We will start from the 10th volume.\n",
    "# For this, knowing that there are originally 225 and that you want to throw away the first 10, please fill in\n",
    "# the following variables\n",
    "start_vol = 10 # Where should we start? (First volume is 0, not 1 !)\n",
    "number_of_volumes = 215 # How many volumes should we keep?\n",
    "\n",
    "fslroi(file_to_realign, output_target, str(start_vol), str(number_of_volumes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b61d3",
   "metadata": {},
   "source": [
    "So, take-away message of this section which is also the point of all these preprocessing steps: <u>always look at your data!</u>.\n",
    "Let's go back to our original dataset now. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524c3e5",
   "metadata": {},
   "source": [
    "### Motion correction\n",
    "\n",
    "Motion correction here specifically means trying to make it such that a given voxel describes the same brain position in all volumes.\n",
    "\n",
    "To illustrate why it might be a good idea, let's have a look at the functional data of our participant. Watch the movie. Do you notice anything strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62c38c",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/motion.gif\"/>\n",
    "    <p style=\"text-align:center;\"><i>You might want to pay attention to the axial view (right)</i></p></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb949fa5",
   "metadata": {},
   "source": [
    "The volumes tend to move a bit around, don't they?\n",
    "<an example moment of motion>\n",
    "    \n",
    "This is a problem. Indeed, when we talk of a given voxel, our hope for analysis is that it represents a specific coordinate of anatomy. Imagine if you were trying to find your way with Google Maps, but every now and then the houses would suddenly all move by one kilometer! Would not be so easy to get to the right address, would it? Well, here it's the same. We want that a given (X, Y, Z) position describes always the same portion of the brain, otherwise our analysis will simply not work.\n",
    "\n",
    "But because of motion, this is not the case.\n",
    "    \n",
    "This is one of the core issues of fMRI: the participant simply moved, ever so slightly, during the acquisition. As a consequence, well, we have a recording of a moving participant. This is not a rare phenomenon: imagine having to keep your head perfectly still for several minutes and you'll quickly understand that it is **hard**!\n",
    "    \n",
    "Still, we would like to do something about it. This is where motion correction steps in. There are two sides to motion correction. The first, which we'll cover here, attempts to put all volumes back in alignment, so that a given position is indeed consistently describing the same anatomical part for all volumes. The second, which you'll see next week, attempts to correct for the consequences of motion on the magnetic field.\n",
    "<br><br>\n",
    "Do you remember Ducky? Well, imagine now that our dear duck has a rare shaking disease.\n",
    "    <br><img src=\"imgs/ducky/shakyducky.png\" style=\"width:auto;height:500px;\"/>\n",
    "<br>If we take several consecutive pictures of Ducky, it won't be as aligned as it should be\n",
    "    <br><img src=\"imgs/ducky/duckies_before_reg.png\" style=\"width:900px;height:auto;\"/>\n",
    "<br>To correct this, I can apply the idea we used in normalization. Let's pick one Ducky image as reference. Then, all other images of Ducky will be registered to this Ducky\n",
    "    <br><img src=\"imgs/ducky/duckies_reg.png\" style=\"width:900px;height:auto;\"/>\n",
    "    <br><img src=\"imgs/ducky/duckies_after_reg.png\" style=\"width:900px;height:auto;\"/>\n",
    "<br>Now, what if I want to remember how much Ducky had moved ? Well, I can remember the parameters of the transformation I had to apply to align the volumes. This fully encapsulates the motion information.\n",
    "\n",
    "This is precisely what motion correction sets out to achieve. For this, we need first to define a reference, if possible in fMRI space and that would not require too much transformations. Which option(s) seem reasonable to you?:\n",
    "\n",
    "- [ ] Choosing as reference a volume of the fMRI timeserie\n",
    "- [ ] Averaging the fMRI timeserie and using the mean volume as reference\n",
    "- [ ] Choosing as reference an anatomical volume, preferably of T1 contrast\n",
    "- [ ] Choosing as reference an fMRI standard space volume, derived from a cohort of participants\n",
    "\n",
    "\n",
    "It turns out the first two options are usually equivalent. Because it saves us one pass of average computation, we will choose the first option: picking a volume and using it as reference! Which one do you think would be good?\n",
    "- [ ] The first volume of the timeserie\n",
    "- [ ] The last volume of the timeserie\n",
    "- [ ] The middle volume of the timeserie\n",
    "- [ ] Any volume such that the bold had the time to settle down\n",
    "\n",
    "Now, let us perform this step, on our **first** dataset (the one without fieldmaps). In FSL, we use <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\">MCFLIRT</a> to perform this correction.\n",
    "<br>\n",
    "    \n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    By default, MCFLIRT selects the middle volume of the EPI serie as reference to which other volumes are realigned.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40bbc5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path_original_data \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241m.\u001b[39mjoin(bids_root_fmap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-001\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-001_task-sitrep_run-01_bold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m path_moco_data \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mjoin(preproc_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-001\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-001_task-sitrep_run-01_bold_moco\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m mcflirt(infile\u001b[38;5;241m=\u001b[39mpath_original_data,o\u001b[38;5;241m=\u001b[39mpath_moco_data, plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, report\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, mats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'op' is not defined"
     ]
    }
   ],
   "source": [
    "path_original_data = os.path.join(bids_root_fmap, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "path_moco_data = os.path.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco')\n",
    "mcflirt(infile=path_original_data,o=path_moco_data, plots=True, report=True, dof=6, mats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76df2e",
   "metadata": {},
   "source": [
    "Okay! So, what do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce43530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root_fmap, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764de111",
   "metadata": {},
   "source": [
    "In the functional folder, notice that we have two new files:\n",
    "```\n",
    "sub-001_task-sitrep_run-01_bold_moco.nii.gz\n",
    "sub-001_task-sitrep_run-01_bold_moco.par\n",
    "\n",
    "```\n",
    "\n",
    "The first one is the corrected EPI time serie, with volumes realigned. The second is a file describing the motion parameters that were used to move each volume. It will be useful very shortly to determine which volume moved by a lot.\n",
    "Notice as well a new directory!\n",
    "```\n",
    "sub-001_task-sitrep_run-01_bold_moco.mat/\n",
    "```\n",
    "This directory is full of .MAT files. These are the transformation matrices used for every volume to realign them.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    The motion parameters and the transformation matrices are related, but they are not exactly the same thing. While you can recover one from the other, it is not trivial. Applying the transformation matrix to a volume will put it 'in alignment' as you've done with FLIRT. However the motion parameters cannot be applied directly. Loosely, the motion parameters describe how you would move if you first applied a rotation along x, then along y, then along y, followed by transition along x, then transition along y, then transition along z. This ordering of transformations is **not** really what happens with the transformation matrices. It is a convention adopted by FSL to make it easier to decouple transformations and rotations in the motion parameter analysis; it is therefore a <b>convenience</b>.\n",
    "    <u>Do not confuse transformation matrices and motion parameters</u>!</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea54013",
   "metadata": {},
   "source": [
    "Before going <u>any further</u>, go and have a look at the corrected timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e78d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load(path_original_data)\n",
    "load(path_moco_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa868cba",
   "metadata": {},
   "source": [
    "Did mcflirt help correct motion? Are you convinced it did somewhat a proper job?\n",
    "<br>\n",
    "It's actually not too easy to tell right? Well, let's see if we can figure something out to ease our quality control!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd66b00",
   "metadata": {},
   "source": [
    "#### Motion parameters and degrees of freedom\n",
    "\n",
    "We told you earlier that motion parameters can be used to estimate the motion along every axis.\n",
    "\n",
    "In our invocation of mcflirt, notice the following:\n",
    "```python\n",
    "mcflirt(..., dof=6)\n",
    "```\n",
    "dof stands for <i><b>d</b>egrees <b>o</b>f <b>f</b>reedom</i>, it really means what kind of transformation we wish to apply. In a 3D transformation, we have 3 axis:\n",
    "<img src=\"imgs/3d_axis.png\"/>\n",
    "Along each axis, we can apply one transformation. Because we apply here only **affine** transformations, we can choose any transformation from:\n",
    "- Translation along the axis\n",
    "- Rotation along the axis\n",
    "- Shear along the axis\n",
    "- Scale along the axis\n",
    "\n",
    "Together, you can see this gives in total **12** DOF.\n",
    "We've chosen 6 DOFs, which is the standard choice: we want only to translate and rotate around the volumes, since they've been displaced by motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4679f33",
   "metadata": {},
   "source": [
    "#### Looking at the resulting correction parameters\n",
    "Recall the motion parameters are stored in the .par file produced by MCFLIRT. Notice that since each volume moved differently, we have one transformation per volume, thus one set of motion parameters per volume as well. We provide you with a way to load these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mot_params_fsl_6_dof(path):\n",
    "    return pd.read_csv(path, sep='  ', header=None, \n",
    "            engine='python', names=['Rotation x', 'Rotation y', 'Rotation z','Translation x', 'Translation y', 'Translation z'])\n",
    "\n",
    "mot_params = load_mot_params_fsl_6_dof(op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco.par'))\n",
    "mot_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932aaae0",
   "metadata": {},
   "source": [
    "Based on **translation on X alone**, can you find perhaps a volume which exceeds with respect to the **preceding volume** a 0.2 mm displacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here to inspect quickly the translation on X :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b807b",
   "metadata": {},
   "source": [
    "Some metrics have been created, to compute the displacement of a frame compared to the preceding frame: this is the frame-wise displacement. <br>(see <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3254728/\">Power, Jonathan D., et al. \"Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion.\" Neuroimage 59.3 (2012): 2142-2154.</a> for more details).<br>\n",
    "We can use this one to extract an aggregate measure of motion for all volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ded6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FD_power(mot_params):\n",
    "    framewise_diff = mot_params.diff().iloc[1:]\n",
    "\n",
    "    rot_params = framewise_diff[['Rotation x', 'Rotation y', 'Rotation z']]\n",
    "    # Estimating displacement on a 50mm radius sphere\n",
    "    # To know this one, we can remember the definition of the radian!\n",
    "    # Indeed, let the radian be theta, the arc length be s and the radius be r.\n",
    "    # Then theta = s / r\n",
    "    # We want to determine here s, for a sphere of 50mm radius and knowing theta. Easy enough!\n",
    "    \n",
    "    # Another way to think about it is through the line integral along the circle.\n",
    "    # Integrating from 0 to theta with radius 50 will give you, unsurprisingly, r0 theta.\n",
    "    converted_rots = rot_params*50\n",
    "    trans_params = framewise_diff[['Translation x', 'Translation y', 'Translation z']]\n",
    "    fd = converted_rots.abs().sum(axis=1) + trans_params.abs().sum(axis=1)\n",
    "    return fd\n",
    "\n",
    "fd = compute_FD_power(mot_params).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c15398",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.quantile(fd,0.75) + 1.5*(np.quantile(fd,0.75) - np.quantile(fd,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eba455",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(list(range(1, fd.size+1)), fd)\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('FD displacement (mm)')\n",
    "plt.hlines(threshold, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259fc83",
   "metadata": {},
   "source": [
    "Okay great, but what if we want to know which volumes are actually above threshold? Simply run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(fd > threshold)[0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c45647",
   "metadata": {},
   "source": [
    "So, you now know which volumes might present motion that is worth checking. Go back to FSLeyes and contrast the uncorrected volumes with the corrected ones. Can you see what sort of motion was problematic and was eliminated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828fa09",
   "metadata": {},
   "source": [
    "### Motion-correction: conclusions\n",
    "\n",
    "Motion correction should always be conducted. As you've seen, it is extremely easy to do and has many benefits. However it is not infaillible. High motion tends to cause non linear effects in the signal that simple motion correction above cannot correct since it has no awareness of the magnetic field. <br>\n",
    "<br> Motion parameters can, in this case, come to our rescue. As they represent the effect of motion, including them in our modeling to try and correct the signal can help. One could for example include this information in a General Linear Model to regress out the signal of these volumes (censoring) from overall timeseries. ‚û°Ô∏è More on this next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ee5b",
   "metadata": {},
   "source": [
    "### Where are we?\n",
    "\n",
    "So, let's see what we have done so far:\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the b0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM next week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "As mentioned earlier:\n",
    "- Doing motion correction or slice-timing first is still a matter of debate in the literature\n",
    "- Field unwarping and coregistration (which you'll see now) can be conducted jointly to improve results. It means that a typical pipeline would actually be in the order: Volumes removal > (Motion correction > slice-timing correction) > Coregistration + Fieldmap unwarping > ...\n",
    "\n",
    "\n",
    "### Coregistration\n",
    "\n",
    "Just what is coregistration? Well, it is basically a registration between images of different modalities. In our specific case, we want to register fMRI (EPI) to an anatomical image (T1). There are several reasons for this. The first that comes to mind is that if you overlay your fMRI on the anatomy, you can of course reason much more easily on where you are in the brain, what activations you might be looking at and so forth. Imagine a participant has a brain lesion visible on the anatomy and you want to see how this reflects on the fMRI. Being able to put the two together would make it much easier, would it not?\n",
    "\n",
    "This is the first reason behind coregistration.\n",
    "\n",
    "The second is because of normalization. Assume you want to compare all fMRI data of participants. Clearly, putting all of them into a common reference frame is a bit trickier, because of how noisy and low-resolution the data is, right? But you know how to map the anatomical to this common space with excellent accuracy, and you've saved this transformation earlier.\n",
    "If you could figure out how to go from the fMRI space to anatomical, clearly the problem would be solved! You'd only have then to apply the transformation from anatomical to common space and be done with it.\n",
    "\n",
    "\n",
    "Computing the fMRI space to anatomical transformation is precisely the goal of coregistration.\n",
    "<br><br>\n",
    "To do this step, we will use a wonderful command: <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT/UserGuide#epi_reg\">epi_reg</a> ! As the name states, it is a command to register an EPI. Hard to make it clearer huh? \n",
    "\n",
    "#### What to do\n",
    "\n",
    "Notice that we want to compute the transformation to use for coregistration.\n",
    "Now, we have an EPI, here of 364 volumes, each supposedly aligned by motion-correction. How many times should we compute the transformation?\n",
    "- [ ] 364 times, once for each volume\n",
    "- [ ] Once, selecting any volume from the EPI\n",
    "\n",
    "Your task is simple. You should:\n",
    "- Fill in the name of the EPI target. It should be the **motion-corrected** EPI that you corrected using MCFLIRT (ignore the slice-timing corrected volume). If you want to use a single volume, set the use_first_vol variable to True!\n",
    "- Fill in the path to the whole head T1 image (**before** skull stripping was conducted!)\n",
    "- Fill in the path to the skull-stripped T1 image (**after** skull stripping was conducted!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0d517",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Make sure that the whole head T1 and the skull-stripped T1 have the same orientation.\n",
    "For example, if you ran fsl_anat to extract the brain (which is fine), FSL will change in the headers the orientation of the T1 before skull-stripping. As a consequence, the brain-extracted T1 no longer has the same orientation as the original T1. If you display them on top of each other, they are perfectly matched, but not from the perspective of the <b>headers</b>, which can play nasty tricks on you when performing coregistration.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f383c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_target = ???\n",
    "use_first_vol = ???\n",
    "whole_t1 = ???\n",
    "skull_stripped_t1 = ???\n",
    "output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_bbr')\n",
    "ref_vol_name =  op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_moco_vol_middle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_single_vol:\n",
    "    # Extract the middle volume with fslroi as we've seen before :)\n",
    "    fslroi(epi_target, ref_vol_name, str(182), str(1))\n",
    "    # Call epi_reg\n",
    "    epi_reg(ref_vol_name, whole_t1, skull_stripped_t1, output_path)\n",
    "    # Delete the first volume (we don't need it anymore :0)\n",
    "else:\n",
    "    epi_reg(epi_target, whole_t1, skull_stripped_t1, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b13492",
   "metadata": {},
   "source": [
    "Notice how FAST is run?\n",
    "This is because the specific coregistration cost (boundary-based registration, BBR) uses the anatomical white-matter tissues from FAST. If no such tissue is provided to the function, it re-runs FAST to obtain it and use it. If you've already done anatomical segmentation, clearly there's no need to redo it right?\n",
    "In particular, imagine if you had to yourself correct the white matter with the help of an expert because somehow FSL did not do a poor job on your data. Clearly you'd like to have this one used instead of the result from FAST, right?\n",
    "\n",
    "Well- you can! We just need a new option in the epi_reg command:\n",
    "```python\n",
    "epi_reg(...,wmseg=path_to_your_white_matter_segmentation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f657883",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_reg(ref_vol_name, whole_t1, skull_stripped_t1, output_path, wmseg=op.join(preproc_root, 'sub-001', 'anat', 'T1_fast_pve_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665690c",
   "metadata": {},
   "source": [
    "Let's overlay the two (EPI and anatomical) on top of each other to visualize the quality of the coregistration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcaf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_overlays()\n",
    "load(skull_stripped_t1)\n",
    "load(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff9a10",
   "metadata": {},
   "source": [
    "Now, how do we *know* if the registration is good or bad?\n",
    "Well, there are several things to watch out for, but here are some main leads:\n",
    "- Is the functional in the right orientation?\n",
    "- Are the ventricles correctly aligned?\n",
    "- Are the boundaries of the EPI more or less matching the anatomical?\n",
    "\n",
    "‚û°Ô∏è You can also check how the white matter of the EPI matches your anatomical's white matter provided you have sufficient resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c4389",
   "metadata": {},
   "source": [
    "#### Some cleanup\n",
    "If you have a look, you might notice that perhaps your directory got filled with many files. These are temporary files, created but uncorrectly not eliminated by epi_reg. The following should help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_epi_reg(path_to_clean):\n",
    "    patterns = ['*_fast_*', '*_fieldmap*']\n",
    "    for p in patterns:\n",
    "        files = glob.glob(op.join(path_to_clean, p))\n",
    "        for f in files:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_epi_reg(op.join(preproc_root, 'sub-001', 'func'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efca3d",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "All these transforms are not exactly perfect. As you've seen in class, a step of smoothing is typically applied, with the size of the smoothing being dependent on your application, starting resolution etc.\n",
    "The idea of smoothing is really that, as you're averaging, hopefully you increase the signal to noise ratio. <br>\n",
    "A side-effect is that finest patterns of activation will be lost in the averaging (we can't have everything: there's no free lunch).\n",
    "\n",
    "With FSL, smoothing is rather easy to do. However, one thing which is important is the size of your filter.\n",
    "Different softwares might use different conventions. For MRI, it is typical to talk about FWHM (Full-width at half maximum), expressed in mms.\n",
    "\n",
    "FSL, however, takes as input in sigma instead of FWHM. The conversion is easy fortunately:\n",
    "\n",
    "$$ \\sigma = \\frac{FWHM}{2.3548}$$\n",
    "\n",
    "Here for example would be the smoothing command for 6mm FWHM smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'fslmaths {} -kernel gauss {} {}_smoothed-6mm'.format(total_epi, 6/2.3548, total_epi)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c06387",
   "metadata": {},
   "source": [
    "Let's observe what we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691354b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(total_epi + '_smoothed-6mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb3abe",
   "metadata": {},
   "source": [
    "Do you feel as though the signal-to-noise ratio was improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4224b",
   "metadata": {},
   "source": [
    "## MRI + fMRI preprocessing: summary\n",
    "\n",
    "So, these were all the steps you were meant to study.\n",
    "\n",
    "You should know by now: preprocessing is extremely important and you will likely spend a lot of time on it. Decisions in preprocessing will affect your analysis, so do not take this step lightly, it is <u>critical</u> to do it as well as possible!\n",
    "\n",
    "<u>Always perform quality control to ensure everything is okay!</u>\n",
    "\n",
    "Let's review one last time the different steps you've studied and which FSL tool(s) you used to do it:\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT + FNIRT (from last week)</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the b0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM next week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Coregistration to anatomical</td><td style='text-align:justify;'>Putting the functional volumes in anatomical space</td><td style='text-align:justify;'>FLIRT (epi_reg being a specialized instance)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Smoothing</td><td style='text-align:justify;'>Allowing a bit of lee-way in the voxel's values to account for the imperfection of the registration</td><td style='text-align:justify;'>fslmath with smoothing operation</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e7792",
   "metadata": {},
   "source": [
    "# Running it all: fmriprep\n",
    "\n",
    "Performing all of these by hands would be tedious as you can imagine.\n",
    "Furthermore, exact choices, bugs, softwares could lead to a lack of reproducibility in your research. For this reason (and many others), the neuroscientific community developed a suite of softwares to conduct all preprocessing steps in an automated fashion, called fmriprep!\n",
    "\n",
    "You actually already launched this step, in the step 0.5 !\n",
    "The command you used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"docker run -ti --rm -v {}:/data:ro -v {}:/out -v /home/NX421:/license_path:ro nipreps/fmriprep:latest /data /out/fmiprep_results participant --participant-label {} --fs-license-file /license_path/license.txt\".format(bids_root, \n",
    "                                                                                                                                               deriv_root, \"001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f147955-432f-40cc-b528-5452a173fded",
   "metadata": {},
   "source": [
    "Several things to note.\n",
    "\n",
    "- We go through a container technology called docker, such that fmriprep and all its subprograms come in bundled and we don't have to compile anything. Very handy!\n",
    "- We must mount the paths where  docker has to look for data and licenses. These are the -v commands. Any path not mounted will not be visible to docker!\n",
    "- We point to the root path of the unprocessed data for docker to fetch data (expected to be BIDS compliant!)\n",
    "- The derivative path is where we store the output\n",
    "- We specified here a participant label. Had we not done so, fmriprep would apply the preprocessing to all participants in the folder. Pay attention to the --participant-label flag when you want to preprocess only one participant in a study!\n",
    "- For a subject named sub-SOMENAME, the subject ID is SOMENAME, not sub-SOMENAME\n",
    "\n",
    "Beware! Although it is easy, it will still take some time depending on your CPU, the volume of data to be preprocessed etc. fMRIprep is here ran through a Docker container. We will not detail docker too much, as it is out of scope for this class. All you need to know is that fmriprep and all its subprograms this way come as bundle, avoiding the hassle of painful installations :)\n",
    "\n",
    "You can nonetheless get more information on all potential options of fmriprep like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ebbf3-3408-43ba-a859-34b3d65e2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"docker run -ti --rm -v {}:/data:ro -v {}:/out fmriprep:latest --help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f88eed",
   "metadata": {},
   "source": [
    "Once fmiprep is done (check back the terminal where you started fmripre, if done it will not write anything anymore!), navigate to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(deriv_root,\"fmriprep_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f203ce",
   "metadata": {},
   "source": [
    "You should see index.html, click it:\n",
    "\n",
    "This will open a new page in firefox to read the hmtl file. Within this page, you will see all results of the preprocessing, including QC steps for you to visually inspect that all went well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d501e6-f2a0-46f6-a20e-0a49a974e476",
   "metadata": {},
   "source": [
    "We **strongly** encourage you for the rest of the class to use fmriprep whenever you can, as it will make your life easier. \n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> üí° Pay attention ! üí°</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    fmriprep is an amazing tool, but it is still not magic. You will need to still inspect your data to make sure everything is fine. Furthermore, you might need to play with fmriprep parameters, such as the template space for the normalization, but also potentially specify a template for the skull stripping, degrees of freedom for the coregistration...You should never use it as a black box and just be done with it! Be critical of what you get.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ce68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
